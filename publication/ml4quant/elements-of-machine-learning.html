<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 2 Elements of Machine Learning | Machine Learning for Quantitative Investment</title>
<meta name="author" content="Fintelligence">
<meta name="description" content="In a minimalism style, machine learning refers to how computers can ‚Äúlearn‚Äù by finding patterns in data and using them to make predictions. Mathematically, given a real-valued output \(y\) and...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 2 Elements of Machine Learning | Machine Learning for Quantitative Investment">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/bg-quant.png">
<meta property="og:description" content="In a minimalism style, machine learning refers to how computers can ‚Äúlearn‚Äù by finding patterns in data and using them to make predictions. Mathematically, given a real-valued output \(y\) and...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 2 Elements of Machine Learning | Machine Learning for Quantitative Investment">
<meta name="twitter:description" content="In a minimalism style, machine learning refers to how computers can ‚Äúlearn‚Äù by finding patterns in data and using them to make predictions. Mathematically, given a real-valued output \(y\) and...">
<meta name="twitter:image" content="/images/bg-quant.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning for Quantitative Investment</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="active" href="elements-of-machine-learning.html"><span class="header-section-number">2</span> Elements of Machine Learning</a></li>
<li><a class="" href="modern-factor-investing.html"><span class="header-section-number">3</span> Modern Factor Investing</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="elements-of-machine-learning" class="section level1" number="2">
<h1>
<span class="header-section-number">2</span> Elements of Machine Learning<a class="anchor" aria-label="anchor" href="#elements-of-machine-learning"><i class="fas fa-link"></i></a>
</h1>
<p>In a minimalism style, machine learning refers to how computers can ‚Äúlearn‚Äù by finding patterns in data and using them to make predictions. Mathematically, given a real-valued output <span class="math inline">\(y\)</span> and predictors vector <span class="math inline">\(X\)</span> containing <span class="math inline">\(p\)</span> variables, we assume a general function <span class="math inline">\(f\)</span> to describe <span class="math inline">\(y\)</span>:</p>
<p><span class="math display" id="eq:ml01">\[\begin{equation} 
y = f(X) + \epsilon
\tag{2.1}
\end{equation}\]</span></p>
<p>Where, <span class="math inline">\(f\)</span> is a fixed but unknown function, and <span class="math inline">\(\epsilon\)</span> is a zero-mean random error term, which is supposed to be independent of <span class="math inline">\(X\)</span>.</p>
<p>The core of machine learning is a suite of data-driven algorithms for estimating <span class="math inline">\(f\)</span>. ML is based on <span style="color:#504EFF"><em>statistical learning theory</em></span> to design models to understand patterns and employs <span style="color:#504EFF"><em>optimization algorithms</em></span> to train the model to ‚Äúlearn‚Äù the pattern using input data.</p>
<p>The foundation of practical machine learning is the data. Data drives everything else. The model can not learn much pattern without enough data and could even have biased behaviour if the data quality is poor. In contrast, with substantial data, the machine learning model could achieve impressive results beyond expectation. A vivid example is to check the google‚Äôs search engine - it is machine learning algorithm under the hood.</p>
<blockquote>
<p>‚ÄúAll models are wrong, but some are useful.‚Äù - George E.P. Box</p>
</blockquote>
<div id="linear-regression" class="section level2" number="2.1">
<h2>
<span class="header-section-number">2.1</span> Linear Regression<a class="anchor" aria-label="anchor" href="#linear-regression"><i class="fas fa-link"></i></a>
</h2>
<p>Linear regression adopts a linear function<span class="math inline">\(f^{Linear}\)</span> to equation <a href="elements-of-machine-learning.html#eq:ml01">(2.1)</a> with learnable model parameters <span class="math inline">\(\beta^T =(\beta_0,\beta_1,...,\beta_p) \in (p,1)\)</span>:</p>
<p><span class="math display" id="eq:lm01">\[\begin{equation} 
y = \beta^T X + \epsilon  
\tag{2.2}
\end{equation}\]</span></p>
<p>Here we assume the first column of <span class="math inline">\(X\)</span> is all ones as an ‚Äúintercept feature‚Äù and thus <span class="math inline">\(\beta_0\)</span> corresponds to the intercept term.</p>
<p>The model given by <a href="elements-of-machine-learning.html#eq:lm01">(2.2)</a> defines the <span style="color:#504EFF"><em>population regression line</em></span>, which is the best linear approximation to the true relationship between <span class="math inline">\(X\)</span> and <span class="math inline">\(u\)</span>. The population regression line is unobserved and we have access to a set of sample observations to compute an sample-based estimate line. Fundamentally, we apply a standard statistical approach of using information from a sample to estimate characteristics of a large population. Here the linear regression focus on the estimate of coefficient <span class="math inline">\(\hat{\beta}\)</span>.</p>
<p>The most popular estimation method is <span style="color:#504EFF"><em>least squares</em></span>, also known as Ordinary Least Squares (OLS). OLS regression is an estimated model based on sample data in which we pick the coefficients <span class="math inline">\(\hat{\beta}\)</span> to minimize the residual sum of squares (RSS):</p>
<p><span class="math display" id="eq:lmrss">\[\begin{equation} 
RSS(\beta) = \sum_{i=1}^N (y_i - f(x_i))^2 = (y-X\beta)^T (y-X \beta)  
\tag{2.3}
\end{equation}\]</span></p>
<p>To minimize <a href="elements-of-machine-learning.html#eq:lmrss">(2.3)</a>, we differentiate the term with respect to <span class="math inline">\(\beta\)</span> and obtain:</p>
<p><span class="math display">\[
\frac{\partial RSS}{\partial \beta} = -2X^T (y - X \beta) \\
\frac{\partial RSS}{\partial \beta \partial \beta^T} =  2 X^TX
\]</span></p>
<p>Assuming that <span class="math inline">\(X\)</span> has full column rank, hence <span class="math inline">\(X^TX\)</span> is positive definite leading to optimality, we thus could set the first derivative to zero to get the <span class="math inline">\(\hat{\beta}\)</span>:</p>
<p><span class="math display">\[
X^T (y - X \beta) = 0  \\
\Rightarrow  X^T y  =  X^T X \hat{\beta}
\]</span></p>
<p>We thus solve the unique solution:</p>
<p><span class="math display" id="eq:lmbeta">\[\begin{equation} 
\hat{\beta} = (X^TX)^{-1} X^T y
\tag{2.4}
\end{equation}\]</span></p>
<p>Please note all this derivation makes no assumptions about the validity of model - it simply finds the best linear fit to the data except from ensuring <span class="math inline">\(X\)</span> is of full-rank. The non-full-rank case occurs most often when there is redundancy. A natural way to resolve the non-unique representation is to drop redundant columns in <span class="math inline">\(X\)</span>.</p>
<p>Lastly, since <span class="math inline">\(\hat{y}=X \hat{\beta}\)</span>, we could form the predictions
<span class="math display" id="eq:lmyfit">\[\begin{equation} 
\hat{y} = X \hat{\beta} = (X(X^TX)^{-1} X^T) y
\tag{2.5}
\end{equation}\]</span></p>
<p>We denote <span class="math inline">\(P=X(X^TX)^{-1} X^T\)</span> which is projection matrix. Moreover, we could represent the residual <span class="math inline">\(\hat{\epsilon}\)</span> with <span class="math inline">\(P\)</span>:
<span class="math display" id="eq:lmresid">\[\begin{equation} 
\hat{\epsilon} = y - X \hat{\beta} = y - (X(X^TX)^{-1} X^T) y = (I_n - P) y
\tag{2.6}
\end{equation}\]</span></p>
<p>Note <span class="math inline">\(Q=I_n - P\)</span> is also a projection matrix. There are nice properties of <span style="color:#504EFF"><em>projection matrices</em></span> that we will use in the subsequent sections:
<span class="math display">\[
Q^T=Q; \;Q^2=Q
\]</span></p>
<div id="sampling-uncertainty-and-statistical-inference" class="section level3" number="2.1.1">
<h3>
<span class="header-section-number">2.1.1</span> Sampling Uncertainty and Statistical Inference<a class="anchor" aria-label="anchor" href="#sampling-uncertainty-and-statistical-inference"><i class="fas fa-link"></i></a>
</h3>
<p>We infer from <a href="elements-of-machine-learning.html#eq:lmbeta">(2.4)</a> that both <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{Y}= X \hat{\beta}\)</span> are linear transformations of <span class="math inline">\(y\)</span> as random variables. If we had collected different sample data of size <span class="math inline">\(n\)</span>, we would to be sure have different estimates, due to <span style="color:#504EFF"><em>sampling uncertainty</em></span>. We want to quantify this uncertainty by using <span style="color:#504EFF"><em>sampling statistics</em></span> and essentially get more control of our estimate coefficient <span class="math inline">\(\hat{\beta}\)</span>. Furthermore, the inclusion of uncertainty measure such as standard error enables us to conduct statistical inference.</p>
<p>To pin down the sampling properties of <span class="math inline">\(\hat{\beta}\)</span>, we include supplementary assumptions including <span style="color:#504EFF"><em>uncorrelated residual</em></span>, <span style="color:#504EFF"><em>constant variance of residual <span class="math inline">\(\sigma^2\)</span></em></span>, and additionally <span style="color:#504EFF"><em><span class="math inline">\(X\)</span> is non-random</em></span>. Based on the assumption, we infer the variance of <span class="math inline">\(y\)</span>:</p>
<p><span class="math display" id="eq:lmyvar">\[\begin{equation} 
Var(y)=\sigma^2 I_n
\tag{2.7}
\end{equation}\]</span></p>
<p>We could then derive the expectation of residual sum of squares (RSS):</p>
<p><span class="math display">\[
RSS = Y^T(1-P)Y  \\
\Rightarrow  E[ Y^T(1-P)Y ] = (n-p) \sigma^2
\]</span></p>
<p>Hence we get the unbiased estimate of variance <span class="math inline">\(\hat{\sigma}^2 = \frac{RSS}{n-p}\)</span>. We next derive the variance-covariance matrix of the <span class="math inline">\(\hat{\beta}\)</span> using equation <a href="elements-of-machine-learning.html#eq:lmyvar">(2.7)</a>:
<span class="math display">\[
\begin{aligned}
Var( \hat{\beta}) &amp;= Var( X(X^TX)^{-1} X^T ) \\&amp;= X(X^TX)^{-1} X^T Var(y) (X(X^TX)^{-1} X^T)^T \\&amp;=\sigma^2 (X^TX)^{-1}
\end{aligned}
\]</span>
Therefore, the uncertainty in individual estimate <span class="math inline">\(\beta_j\)</span> is quantified by its <span style="color:#504EFF"><em>standard error (<span class="math inline">\(SE\)</span>)</em></span>
<span class="math display">\[
\begin{aligned}
SE(\beta_j) = \sqrt{ \frac{\sigma}{ (X^TX)^{-1}_{jj} }}
\end{aligned}
\]</span></p>
</div>
<div id="rethinking-the-unbiasedness-of-ols" class="section level3" number="2.1.2">
<h3>
<span class="header-section-number">2.1.2</span> Rethinking the Unbiasedness of OLS<a class="anchor" aria-label="anchor" href="#rethinking-the-unbiasedness-of-ols"><i class="fas fa-link"></i></a>
</h3>
<p>We require a property of unbiasedness for estimator <span class="math inline">\(\hat{B}\)</span>. Unbiased estimator does not systematically over- or under-estimate the true parameter <span class="math inline">\(B\)</span>.</p>
<p>Consider the decomposition of mean squared error of an estimator <span class="math inline">\(\tilde{\theta}\)</span> in estimating <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
MSE(\tilde{\theta}) &amp;= E[ \tilde{\theta} - \theta ]^2 \\
                    &amp;= Var( \tilde{\theta} ) +  [E( \tilde{\theta} ) - \theta ]^2
\end{aligned}
\]</span></p>
<p>Here the first term <span class="math inline">\(Var( \tilde{\theta} )\)</span> is the variance, while the second term <span class="math inline">\([E( \tilde{\theta} ) - \theta ]^2\)</span> is the squared bias.</p>
<p>The Gauss-Markov (GM) theorem implies that the least squares estimator has the smallest mean squared error (MSE) of all <span style="color:#504EFF"><em>linear estimators with no bias</em></span> (‚ö†Ô∏è pay attention to the ‚Äúno bias‚Äù here). However, there may well exist a biased estimator with smaller mean squared error. Such an estimator would <span style="color:#504EFF"><em>trade a little bias for a larger reduction in variance</em></span>. Put bluntly, <span style="color:#504EFF"><em>biased estimates are commonly used</em></span> for better out-of-sample performance (i.e., less MSE in test set). In reality, any method that shrinks or sets to zero some of the least squares coefficients may result in a biased estimate.</p>
</div>
</div>
<div id="assumptions-of-linear-regression-and-violation-implications" class="section level2" number="2.2">
<h2>
<span class="header-section-number">2.2</span> Assumptions of Linear Regression and Violation Implications<a class="anchor" aria-label="anchor" href="#assumptions-of-linear-regression-and-violation-implications"><i class="fas fa-link"></i></a>
</h2>
<p>The question for the assumption of linear regression is ill-posed and requires more context as we need to specify the desired properties that we want the linear model to hold. Hence, we start with the standard assumptions that guarantee the above GM theorem to hold for OLS regression estimate:
1. There exists an additive linear model for <span class="math inline">\((X, Y)\)</span> as equation <a href="elements-of-machine-learning.html#eq:ml01">(2.1)</a>,
<span class="math display">\[
Y= X \beta + \epsilon
\]</span>
Where, <span class="math inline">\(\beta\)</span> and <span class="math inline">\(X\)</span> are <span style="color:#504EFF"><em>non-random</em></span> while the randomness stem from <span class="math inline">\(\epsilon\)</span></p>
<ol start="2" style="list-style-type: decimal">
<li><p>Zero Expected Mean of Residual: <span class="math inline">\(E[ \epsilon_i] = 0\)</span></p></li>
<li><p>Homoscedasticity: Constant Variance of Residual: <span class="math inline">\(Var(\epsilon_i)=\sigma^2 &lt; \infty\)</span></p></li>
<li><p>Uncorrelated Residual Error: <span class="math inline">\(Cov(\epsilon_i, \epsilon_j)=0, \; \forall i\neq j\)</span></p></li>
</ol>
<p>Furthermore, if the error terms are 5. normally distributed and 6.identically and independently distributed (i.i.d.), we could infer that the OLS estimator becomes the Maximum Likelihood Estimation (MLE).</p>
<div id="problem-1-non-linearity-of-the-data" class="section level3 unlisted unnumbered">
<h3>Problem 1: Non-linearity of the Data<a class="anchor" aria-label="anchor" href="#problem-1-non-linearity-of-the-data"><i class="fas fa-link"></i></a>
</h3>
<p>The linear regression model assumes that there is a straight-line relationship between the predictors and the response. If the true relationship is far from linear, then all of the conclusions that we draw from the fit are suspect.</p>
<p>üîç <em>Residual plots</em> are a useful visualization tool for identifying non-linearity. Given a fitted linear regression model, we can plot the residuals, <span class="math inline">\(e_i = y_i - \hat{y_i}\)</span>, versus the predictor <span class="math inline">\(x_i\)</span>. In the case of a multivariate regression model, we can plot the residuals versus the fitted values <span class="math inline">\(\hat{y_i}\)</span>. Independence assumption implies no discernible pattern. The presence of a pattern may indicate a problem with some aspect of the linear model. For instance, if residuals exhibit a clear U-shape, which provides a strong indication of non-linearity in the data.</p>
</div>
<div id="problem-2-correlation-of-error-terms" class="section level3 unlisted unnumbered">
<h3>Problem 2: Correlation of Error Terms<a class="anchor" aria-label="anchor" href="#problem-2-correlation-of-error-terms"><i class="fas fa-link"></i></a>
</h3>
<p>An important assumption of the linear regression model is that the error terms <span class="math inline">\({\epsilon_i}\)</span> are uncorrelated. Intuitively, if the errors are uncorrelated, then the fact that <span class="math inline">\(epsilon_i\)</span> is positive provides little or no information about the sign of <span class="math inline">\(\epsilon_{i+1}\)</span>. One counterexample is to think about double a set of training observations <span class="math inline">\((X, y)\)</span>, as <span class="math inline">\(X_{Copy} = [X;X], y_{Copy}=[y;y]\)</span>.</p>
<p>The standard errors that are computed for the estimated regression coefficients or the fitted values are based on the assumption of uncorrelated error terms. If in fact there is correlation among the error terms, then the estimated standard errors will tend to <span style="color:#504EFF">underestimate</span> the true standard errors. As a result, confidence and prediction intervals will be narrower than expected.</p>
<p>Such residual correlations frequently occur in the context of <span style="color:#504EFF">time series data</span>, which consists of observations for which measurements are obtained at discrete points in time. There is the issue of <span style="color:#504EFF">residual <a href="https://en.wikipedia.org/wiki/Autocorrelation">autocorrelation</a></span> - tracking in the residuals that adjacent residuals may have similar values.</p>
<p>In general, the assumption of uncorrelated errors is extremely important for linear regression as well as for other statistical methods, and good experimental design is crucial in order to mitigate the risk of such correlations.</p>
</div>
<div id="problem-3-outliers" class="section level3 unlisted unnumbered">
<h3>Problem 3: Outliers<a class="anchor" aria-label="anchor" href="#problem-3-outliers"><i class="fas fa-link"></i></a>
</h3>
<p>An outlier is a point which is far from the value predicted by the model. Outliers can arise for a variety of reasons, such as incorrect data collection. They are quite common in real dataset.</p>
<p>Residual plots can be used to identify outliers. If we believe that an outlier has occurred due to an error in data collection, then one solution is to simply remove the observation. Shrewd care and revised assumptions should be taken when removing outliers, since an outlier may instead indicate a deficiency with the model, such as a missing predictor. 2008 global financial crisis is a data point that needs justification to be discarded as outlier.</p>
</div>
<div id="problem-4-collinearity" class="section level3 unlisted unnumbered">
<h3>Problem 4: üí•Collinearity<a class="anchor" aria-label="anchor" href="#problem-4-collinearity"><i class="fas fa-link"></i></a>
</h3>
<p>Collinearity refers to the situation in which a group of variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. Collinearity reduces the accuracy of the estimates of the regression coefficients, it causes the standard error for <span class="math inline">\(\hat{\beta_j}\)</span> to grow.</p>
<p>üîç <span style="color:#504EFF">Correlation matrix of the predictors</span> is a simple way to detect collinearity. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinearity to exist between three or more variables even if no pair of variables has a particularly high correlation - this is called <span style="color:#504EFF"><em>multicollinearity</em></span>.</p>
<p>There are in two practical solutions to the problem of collinearity. The first, and more straightforward one, is to <span style="color:#504EFF"><em>drop</em></span> one of the problematic variables from the regression. The second solution is to <span style="color:#504EFF"><em>aggregate</em></span> the collinear variables together into a single composite predictor. For example, apply <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a> (Principal Component Analysis) approach to model the highly correlated features group and extract the first PC component as the representative indicator.</p>
</div>
</div>
<div id="conclusion" class="section level2" number="2.3">
<h2>
<span class="header-section-number">2.3</span> Conclusion<a class="anchor" aria-label="anchor" href="#conclusion"><i class="fas fa-link"></i></a>
</h2>
<p>Linear models were largely developed in the precomputer age of statistics, but there are still robust baseline model to apply in today‚Äôs computer era. They are simple and often provide an adequate and interpretable description of how the inputs affect the output. For prediction purposes they can sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="intro.html"><span class="header-section-number">1</span> Introduction</a></div>
<div class="next"><a href="modern-factor-investing.html"><span class="header-section-number">3</span> Modern Factor Investing</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#elements-of-machine-learning"><span class="header-section-number">2</span> Elements of Machine Learning</a></li>
<li>
<a class="nav-link" href="#linear-regression"><span class="header-section-number">2.1</span> Linear Regression</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#sampling-uncertainty-and-statistical-inference"><span class="header-section-number">2.1.1</span> Sampling Uncertainty and Statistical Inference</a></li>
<li><a class="nav-link" href="#rethinking-the-unbiasedness-of-ols"><span class="header-section-number">2.1.2</span> Rethinking the Unbiasedness of OLS</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#assumptions-of-linear-regression-and-violation-implications"><span class="header-section-number">2.2</span> Assumptions of Linear Regression and Violation Implications</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#problem-1-non-linearity-of-the-data">Problem 1: Non-linearity of the Data</a></li>
<li><a class="nav-link" href="#problem-2-correlation-of-error-terms">Problem 2: Correlation of Error Terms</a></li>
<li><a class="nav-link" href="#problem-3-outliers">Problem 3: Outliers</a></li>
<li><a class="nav-link" href="#problem-4-collinearity">Problem 4: üí•Collinearity</a></li>
</ul>
</li>
<li><a class="nav-link" href="#conclusion"><span class="header-section-number">2.3</span> Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning for Quantitative Investment</strong>" was written by Fintelligence. It was last built on 2022-01-29.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
