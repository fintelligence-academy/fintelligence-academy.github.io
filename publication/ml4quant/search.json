[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Welcome first edition Machine Learning Quantitative Investment (ml4quant)! online book machine learning data science methods quantitative investment presented Fintelligence Academy: https://fintelligence-academy.github.io/.adoption Machine Learning Artificial Intelligence continues progress niche activity mainstream applications ever-accelerating pace Big Data era. Fintelligence, embracing ML financial application.book, aim walk ….quantitative research mind, intended audience presumed interdisciplinary, fluent mathematical notation, familiar basic data science concepts. may surprised neglect econometric/finance elements - confident teach foundational knowledge financial market along reading journey. addition, book also interesting readers thinking joining quant workforce data science community.another vein, book also brings mathematical foundations basic machine learning concepts fore collects information single place reader learns modern quantitative finance machine learning. assimilate benefit core ideas two classical machine learning textbooks Introduction Statistical Learning (ISL2) Elements Statistical Learning (ESL), best -go reference books learn machine learning. ml4quant, strive strike balance difficulty middle level two books season topic new flavor financial data science.genuinely hope quantitative researchers data scientists enjoy reading interdisciplinary book find helpful.“best time plant tree twenty years ago. second best time now.” - Chinese Proverb Poet Wang Bo“东隅已逝，桑榆非晚。”-王渤《滕王阁序》","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Machine learning latest long line attempts distill human knowledge reasoning form suitable constructing computer automated systems. enthusiastic practitioner interested learn magic behind successful machine learning algorithms currently faces daunting set pre-requisite knowledge:Programming languages data analysis toolsProgramming languages data analysis toolsLarge-scale computation associated frameworksLarge-scale computation associated frameworksMathematics statistics machine learning builds itMathematics statistics machine learning builds itMachine learning builds upon language mathematics express concepts seem intuitively obvious surprisingly difficult formalize. formalized properly, can gain insights task want solve.can label chapter section titles using {#label} , e.g., can reference Chapter 1. manually label , automatic labels anyway, e.g., Chapter ??.Figures tables captions placed figure table environments, respectively.\nFigure 1.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 1.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 1.1.Table 1.1: nice table!can write citations, . example, using bookdown package (Xie 2021) sample book, built top R Markdown knitr (Xie 2015).","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"elements-of-machine-learning.html","id":"elements-of-machine-learning","chapter":"2 Elements of Machine Learning","heading":"2 Elements of Machine Learning","text":"minimalism style, machine learning refers computers can “learn” finding patterns data using make predictions. Mathematically, given real-valued output \\(y\\) predictors vector \\(X\\) containing \\(p\\) variables, assume general function \\(f\\) describe \\(y\\):\\[\\begin{equation} \ny = f(X) + \\epsilon\n\\tag{2.1}\n\\end{equation}\\], \\(f\\) fixed unknown function, \\(\\epsilon\\) zero-mean random error term, supposed independent \\(X\\).core machine learning suite data-driven algorithms estimating \\(f\\). ML based statistical learning theory design models understand patterns employs optimization algorithms train model “learn” pattern using input data.foundation practical machine learning data. Data drives everything else. model can learn much pattern without enough data even biased behaviour data quality poor. contrast, substantial data, machine learning model achieve impressive results beyond expectation. vivid example check google’s search engine - machine learning algorithm hood.“models wrong, useful.” - George E.P. Box","code":""},{"path":"elements-of-machine-learning.html","id":"linear-regression","chapter":"2 Elements of Machine Learning","heading":"2.1 Linear Regression","text":"Linear regression adopts linear function\\(f^{Linear}\\) equation (2.1) learnable model parameters \\(\\beta^T =(\\beta_0,\\beta_1,...,\\beta_p) \\(p,1)\\):\\[\\begin{equation} \ny = \\beta^T X + \\epsilon  \n\\tag{2.2}\n\\end{equation}\\]assume first column \\(X\\) ones “intercept feature” thus \\(\\beta_0\\) corresponds intercept term.model given (2.2) defines population regression line, best linear approximation true relationship \\(X\\) \\(u\\). population regression line unobserved access set sample observations compute sample-based estimate line. Fundamentally, apply standard statistical approach using information sample estimate characteristics large population. linear regression focus estimate coefficient \\(\\hat{\\beta}\\).popular estimation method least squares, also known Ordinary Least Squares (OLS). OLS regression estimated model based sample data pick coefficients \\(\\hat{\\beta}\\) minimize residual sum squares (RSS):\\[\\begin{equation} \nRSS(\\beta) = \\sum_{=1}^N (y_i - f(x_i))^2 = (y-X\\beta)^T (y-X \\beta)  \n\\tag{2.3}\n\\end{equation}\\]minimize (2.3), differentiate term respect \\(\\beta\\) obtain:\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2X^T (y - X \\beta) \\\\\n\\frac{\\partial RSS}{\\partial \\beta \\partial \\beta^T} =  2 X^TX\n\\]Assuming \\(X\\) full column rank, hence \\(X^TX\\) positive definite leading optimality, thus set first derivative zero get \\(\\hat{\\beta}\\):\\[\nX^T (y - X \\beta) = 0  \\\\\n\\Rightarrow  X^T y  =  X^T X \\hat{\\beta}\n\\]thus solve unique solution:\\[\\begin{equation} \n\\hat{\\beta} = (X^TX)^{-1} X^T y\n\\tag{2.4}\n\\end{equation}\\]Please note derivation makes assumptions validity model - simply finds best linear fit data except ensuring \\(X\\) full-rank. non-full-rank case occurs often redundancy. natural way resolve non-unique representation drop redundant columns \\(X\\).Lastly, since \\(\\hat{y}=X \\hat{\\beta}\\), form predictions\n\\[\\begin{equation} \n\\hat{y} = X \\hat{\\beta} = (X(X^TX)^{-1} X^T) y\n\\tag{2.5}\n\\end{equation}\\]denote \\(P=X(X^TX)^{-1} X^T\\) projection matrix. Moreover, represent residual \\(\\hat{\\epsilon}\\) \\(P\\):\n\\[\\begin{equation} \n\\hat{\\epsilon} = y - X \\hat{\\beta} = y - (X(X^TX)^{-1} X^T) y = (I_n - P) y\n\\tag{2.6}\n\\end{equation}\\]Note \\(Q=I_n - P\\) also projection matrix. nice properties projection matrices use subsequent sections:\n\\[\nQ^T=Q; \\;Q^2=Q\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"sampling-uncertainty-and-statistical-inference","chapter":"2 Elements of Machine Learning","heading":"2.1.1 Sampling Uncertainty and Statistical Inference","text":"infer (2.4) \\(\\hat{\\beta}\\) \\(\\hat{Y}= X \\hat{\\beta}\\) linear transformations \\(y\\) random variables. collected different sample data size \\(n\\), sure different estimates, due sampling uncertainty. want quantify uncertainty using sampling statistics essentially get control estimate coefficient \\(\\hat{\\beta}\\). Furthermore, inclusion uncertainty measure standard error enables us conduct statistical inference.pin sampling properties \\(\\hat{\\beta}\\), include supplementary assumptions including uncorrelated residual, constant variance residual \\(\\sigma^2\\), additionally \\(X\\) non-random. Based assumption, infer variance \\(y\\):\\[\\begin{equation} \nVar(y)=\\sigma^2 I_n\n\\tag{2.7}\n\\end{equation}\\]derive expectation residual sum squares (RSS):\\[\nRSS = Y^T(1-P)Y  \\\\\n\\Rightarrow  E[ Y^T(1-P)Y ] = (n-p) \\sigma^2\n\\]Hence get unbiased estimate variance \\(\\hat{\\sigma}^2 = \\frac{RSS}{n-p}\\). next derive variance-covariance matrix \\(\\hat{\\beta}\\) using equation (2.7):\n\\[\n\\begin{aligned}\nVar( \\hat{\\beta}) &= Var( X(X^TX)^{-1} X^T ) \\\\&= X(X^TX)^{-1} X^T Var(y) (X(X^TX)^{-1} X^T)^T \\\\&=\\sigma^2 (X^TX)^{-1}\n\\end{aligned}\n\\]\nTherefore, uncertainty individual estimate \\(\\beta_j\\) quantified standard error (\\(SE\\))\n\\[\n\\begin{aligned}\nSE(\\beta_j) = \\sqrt{ \\frac{\\sigma}{ (X^TX)^{-1}_{jj} }}\n\\end{aligned}\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"rethinking-the-unbiasedness-of-ols","chapter":"2 Elements of Machine Learning","heading":"2.1.2 Rethinking the Unbiasedness of OLS","text":"require property unbiasedness estimator \\(\\hat{B}\\). Unbiased estimator systematically - -estimate true parameter \\(B\\).Consider decomposition mean squared error estimator \\(\\tilde{\\theta}\\) estimating \\(\\theta\\):\\[\n\\begin{aligned}\nMSE(\\tilde{\\theta}) &= E[ \\tilde{\\theta} - \\theta ]^2 \\\\\n                    &= Var( \\tilde{\\theta} ) +  [E( \\tilde{\\theta} ) - \\theta ]^2\n\\end{aligned}\n\\]first term \\(Var( \\tilde{\\theta} )\\) variance, second term \\([E( \\tilde{\\theta} ) - \\theta ]^2\\) squared bias.Gauss-Markov (GM) theorem implies least squares estimator smallest mean squared error (MSE) linear estimators bias (⚠️ pay attention “bias” ). However, may well exist biased estimator smaller mean squared error. estimator trade little bias larger reduction variance. Put bluntly, biased estimates commonly used better --sample performance (.e., less MSE test set). reality, method shrinks sets zero least squares coefficients may result biased estimate.","code":""},{"path":"elements-of-machine-learning.html","id":"assumptions-of-linear-regression-and-violation-implications","chapter":"2 Elements of Machine Learning","heading":"2.2 Assumptions of Linear Regression and Violation Implications","text":"question assumption linear regression ill-posed requires context need specify desired properties want linear model hold. Hence, start standard assumptions guarantee GM theorem hold OLS regression estimate:\n1. exists additive linear model \\((X, Y)\\) equation (2.1),\n\\[\nY= X \\beta + \\epsilon\n\\]\n, \\(\\beta\\) \\(X\\) non-random randomness stem \\(\\epsilon\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Furthermore, error terms 5. normally distributed 6.identically independently distributed (..d.), infer OLS estimator becomes Maximum Likelihood Estimation (MLE).","code":""},{"path":"elements-of-machine-learning.html","id":"problem-1-non-linearity-of-the-data","chapter":"2 Elements of Machine Learning","heading":"Problem 1: Non-linearity of the Data","text":"linear regression model assumes straight-line relationship predictors response. true relationship far linear, conclusions draw fit suspect.🔍 Residual plots useful visualization tool identifying non-linearity. Given fitted linear regression model, can plot residuals, \\(e_i = y_i - \\hat{y_i}\\), versus predictor \\(x_i\\). case multivariate regression model, can plot residuals versus fitted values \\(\\hat{y_i}\\). Independence assumption implies discernible pattern. presence pattern may indicate problem aspect linear model. instance, residuals exhibit clear U-shape, provides strong indication non-linearity data.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-2-correlation-of-error-terms","chapter":"2 Elements of Machine Learning","heading":"Problem 2: Correlation of Error Terms","text":"important assumption linear regression model error terms \\({\\epsilon_i}\\) uncorrelated. Intuitively, errors uncorrelated, fact \\(epsilon_i\\) positive provides little information sign \\(\\epsilon_{+1}\\). One counterexample think double set training observations \\((X, y)\\), \\(X_{Copy} = [X;X], y_{Copy}=[y;y]\\).standard errors computed estimated regression coefficients fitted values based assumption uncorrelated error terms. fact correlation among error terms, estimated standard errors tend underestimate true standard errors. result, confidence prediction intervals narrower expected.residual correlations frequently occur context time series data, consists observations measurements obtained discrete points time. issue residual autocorrelation - tracking residuals adjacent residuals may similar values.general, assumption uncorrelated errors extremely important linear regression well statistical methods, good experimental design crucial order mitigate risk correlations.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-3-outliers","chapter":"2 Elements of Machine Learning","heading":"Problem 3: Outliers","text":"outlier point far value predicted model. Outliers can arise variety reasons, incorrect data collection. quite common real dataset.Residual plots can used identify outliers. believe outlier occurred due error data collection, one solution simply remove observation. Shrewd care revised assumptions taken removing outliers, since outlier may instead indicate deficiency model, missing predictor. 2008 global financial crisis data point needs justification discarded outlier.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-4-collinearity","chapter":"2 Elements of Machine Learning","heading":"Problem 4: 💥Collinearity","text":"Collinearity refers situation group variables closely related one another. presence collinearity can pose problems regression context, since can difficult separate individual effects collinear variables response. Collinearity reduces accuracy estimates regression coefficients, causes standard error \\(\\hat{\\beta_j}\\) grow.🔍 Correlation matrix predictors simple way detect collinearity. Unfortunately, collinearity problems can detected inspection correlation matrix: possible collinearity exist three variables even pair variables particularly high correlation - called multicollinearity.two practical solutions problem collinearity. first, straightforward one, drop one problematic variables regression. second solution aggregate collinear variables together single composite predictor. example, apply PCA (Principal Component Analysis) approach model highly correlated features group extract first PC component representative indicator.","code":""},{"path":"elements-of-machine-learning.html","id":"conclusion","chapter":"2 Elements of Machine Learning","heading":"2.3 Conclusion","text":"Linear models largely developed precomputer age statistics, still robust baseline model apply today’s computer era. simple often provide adequate interpretable description inputs affect output. prediction purposes can sometimes outperform fancier nonlinear models, especially situations small numbers training cases, low signal--noise ratio sparse data.","code":""},{"path":"modern-factor-investing.html","id":"modern-factor-investing","chapter":"3 Modern Factor Investing","heading":"3 Modern Factor Investing","text":"give overview modern factor investing chapter. intended introductory chapter help readers get familiar quantitative finance framework. chapter adapted Webinar series Modern Factor Investing: Past, Present, Future.last 50 years, academic researchers made major breakthroughs advancing classical practice finance, fundamental analysis scientific quantitative research. include portfolio theory, corporate finance, financial engineering derivative instruments, many applications pertaining financial markets overall. One foundational pillar advances capital market theory context descriptive equilibrium propositions terms risk/return tradeoff linear factor model explain expected asset price, beginning William F. Sharpe Capital Asset Pricing Model (CAPM) (Sharpe 1964). Many related academic developments provided rich asset pricing insight, including Arbitrage Pricing Theory (APT) (Ross 1976), market efficiency proposition, market anomalies Fama French factor model, behavioral finance. altogether gave birth new investment paradigm - factor investing - classical financial textbooks end.saying, “change constant life” attributed Greek philosopher, Heraclitus around 500BC. even prominent capital market. evolution financial market keeps testing efficacy factor theory model. Ever since 2008 global financial crisis, observe irreversible performance decay traditional factor model solely based conventional data linear framework. academia, also replication crisis (credibility crisis) raging “claimed research findings financial economics likely false.” (Harvey, Liu, Zhu 2016)Perhaps Fourth Industrial Revolution already come, “AI new electricity”. trend, factor models strategies remained static. Factor models investment strategies evolve gradually time, reflecting evolution capital markets advances data science theories, data availability, investment practice. One exciting current innovation efforts factor investing focus exploiting new unstructured data sources - alternative data, applying new data science modeling techniques - machine learning - modern factor investing starts.","code":""},{"path":"modern-factor-investing.html","id":"overview-of-factor-investing","chapter":"3 Modern Factor Investing","heading":"3.1 Overview of Factor Investing","text":"One central themes assets priced rationally, variables related average returns, size book--market equity, must proxy sensitivity common (shared thus undiversifiable) risk factors returns — Fama French (1993)question drives asset returns staple neoclassical finance remains holly grail. factor model emerged became foundation asset pricing theory 1960s (Lintner, 1965; Mossin, 1966; Sharpe, 1964 Treynor, 1961). general, Factor can defined underlying characteristic relating group assets (e.g., equity, bond) linearly significant explaining return risk.Factor investing refer general use factors investment process. trustworthily quantitative paradigm investment factor models renders return prediction signal, forecast expected asset risk, drive asset allocation decisions using tradable factor portfolios. Practically, factor-driven investment strategies smart beta products amount $800 billion AUM worldwide (Johnson 2018) — yes, talking magnificent investment topic may affect everyone’s saving pension.Fundamental factors bedrock modern investment management served needs investors long time. equities, addition countries industries, fundamental factors documented empirical research used extensively portfolio management include value, size, momentum, volatility, quality, yield, growth, liquidity. similar set common macro drivers identified used across asset classes, including equity, rates, credit, real assets.Now mathematical formulation, delve granular categories factors:Beta factors — factors corresponding \\(f_t\\) generally referred “risk premia factors”, reflected exposure sources systematic risk supposed earn persistent significant risk-compensated premium long periods (model assumptions work…).\npractice, called “beta factors” “style factors” denote drivers systematic return risk assets. Furthermore, usually associated systematic risk inherent market.\nrisk premium reward thought selling insurance products Mr. Market. undertake risk receive positive reward average long-term, risk happen, Mr Market come coverage suffer realized loss well anticipated. believe game fair agree philosophy passive investing.Beta factors — factors corresponding \\(f_t\\) generally referred “risk premia factors”, reflected exposure sources systematic risk supposed earn persistent significant risk-compensated premium long periods (model assumptions work…).practice, called “beta factors” “style factors” denote drivers systematic return risk assets. Furthermore, usually associated systematic risk inherent market.risk premium reward thought selling insurance products Mr. Market. undertake risk receive positive reward average long-term, risk happen, Mr Market come coverage suffer realized loss well anticipated. believe game fair agree philosophy passive investing.Alpha factors — factors fall \\(\\alpha_{,t}\\) term explain risk well (e.g., volatile) can included linear terms, yet earn persistent return time predictive value result \\(\\alpha_{,t} \\neq 0\\).\nAlpha factor model designed forecast excess return stocks. return distribution characterized expected return standard deviation, often expected return predicted alphas determines whether buy sell, overweight underweight, standard deviation driven betas determines size portfolio allocations hedge undesirable systematic risk - monologue hedge funds active investment.\nEfficient market theory forbids existence alpha academia call “anomaly”, however, hedge fund active managers rely seem live well. often called “signal” practitioners distinguish beta risk premia factors.\nalpha signals often proprietary highly guarded, reflecting creativity well superior systems. important differentiator within investment firm.Alpha factors — factors fall \\(\\alpha_{,t}\\) term explain risk well (e.g., volatile) can included linear terms, yet earn persistent return time predictive value result \\(\\alpha_{,t} \\neq 0\\).Alpha factor model designed forecast excess return stocks. return distribution characterized expected return standard deviation, often expected return predicted alphas determines whether buy sell, overweight underweight, standard deviation driven betas determines size portfolio allocations hedge undesirable systematic risk - monologue hedge funds active investment.Efficient market theory forbids existence alpha academia call “anomaly”, however, hedge fund active managers rely seem live well. often called “signal” practitioners distinguish beta risk premia factors.alpha signals often proprietary highly guarded, reflecting creativity well superior systems. important differentiator within investment firm.","code":""},{"path":"modern-factor-investing.html","id":"years-of-factor-investing-a-literature-review","chapter":"3 Modern Factor Investing","heading":"3.2 50 Years of Factor Investing: A Literature Review","text":"Factor research prevalent 40 years. oldest well-known model stock returns Capital Asset Pricing Model (CAPM), essence single-factor model market sole factor (see Sharpe 1964). words, expected return stock viewed function beta factor loading market . won William Sharpe Nobel Prize.","code":""},{"path":"modern-factor-investing.html","id":"limitedness-of-classical-factor-model-and-the-factor-zoo","chapter":"3 Modern Factor Investing","heading":"3.3 Limitedness of Classical Factor Model and the Factor Zoo","text":"enduring success fundamental factors stems fact guided academic theory, , importantly, supported empirical evidence reflect investment practice, .e., make money. However, story factors turn horror fiction ever since 2008 Global Financial Crisis. well-acclaimed factors outperform past start underperform, also lurking black swan events causing extreme outliers model fail drastically. statistics usually associated heavy-tailed distribution sometimes remove winsorize data points outliers fitting model. Nevertheless, reality outliers may mean doomed .make concrete example, lets remember David Viniar’s, CFO Goldman Sachs, famous explanation Goldman’s flagship GEO hedge fund lost 27% 2007: “seeing things 25-standard deviation moves, several days row.”. One commentator wryly noted:Viniar. comic. According Goldman’s mathematical models, August, Year Lord 2007, special month. Things happening supposed happen every 100,000 years. Either … Goldman’s models wrong (Bonner, 2007b).direct implication normality assumption linear models hold financial market, thereby significantly weakening statistical inference power quantitative fiance. p-value relevant test statistics thus interpreted extreme caution neglected altogether. empirically recommend latter.importantly, underlying linearity assumption unrealistic assumption witness great amount evidence non-linearity breaking fundamental assumption. One straightforward counterexample interaction effect one factor’s contribution asset’s expected return may rely factors. Furthermore, complex pattern certain factor exerts asset return risk. forget theoretical unbiasedness credit linear model entering quantitative finance.\nFigure 3.1: control factor production, (Harvey, 2019)\npoint multiple testing problem: many factors tried, appear “significant” purely chance, indicates peril data mining, may may intentional. data science perspective, also leads high-dimension nature modern factor investing: now “zoo” available factor candidates traditional OLS linear regression incompetent.Therefore, linear model compromise facing low noise--signal ratio data financial market.","code":""},{"path":"modern-factor-investing.html","id":"modern-factor-investing-in-the-big-data-and-machine-learning-trend","chapter":"3 Modern Factor Investing","heading":"3.4 Modern Factor Investing in the Big Data and Machine Learning Trend","text":"Big Data? wide array information sensing devices well cloud service technique led exponential growth available big datasets. Starting handheld mobiles, cameras, microphones, Internets, digitalised map, Internet Things (IoT), LiDAR (Light Detection Ranging) sensors satellite imaging. amount data world collects experienced explosive growth. rapid development informative technologies paved way transition digital world.rapid advancement Internet social media sectors far reaching effects day day lives; also, finance researchers investors. Quantitative finance longer subclass finance arguably whole finance evolving data-driven scientific subject. comes fashionable term finance domain - alternative data.Alternative data data typically used researchers model builders can sourced kaleidoscope data sources ranging text, audio, image, graph, etc. data sets typically minimal aggregation processing making difficult access use. last several years, seen explosion availability new alternative data sources.alternative data sets market data previously difficult access (e.g., analyst forecasts, insider transactions, options trading information, credit default swaps, hedge fund positions). data sets available numerical format can sourced commercial vendors.alternative data sets market data previously difficult access (e.g., analyst forecasts, insider transactions, options trading information, credit default swaps, hedge fund positions). data sets available numerical format can sourced commercial vendors.new data sets come new atypical formats (e.g., text, audio, video), can extracted internet sites social media. Investors may gain additional insights processing understanding information contained unstructured alternative data sets news reports, product reviews, employee reviews, job postings, regulatory filings, call transcripts, satellite images, etc.new data sets come new atypical formats (e.g., text, audio, video), can extracted internet sites social media. Investors may gain additional insights processing understanding information contained unstructured alternative data sets news reports, product reviews, employee reviews, job postings, regulatory filings, call transcripts, satellite images, etc.new formats require new modeling techniques powerful computers extract information. Innovations information technology cloud computing bespoke microprocessors provide computational firepower necessary apply machine learning computationally intensive data science techniques new unstructured alternative data sets.Natural language processing (NLP) one exciting frontiers bring alternative value factor investing framework. essence, now leverage deep-learning based NLP model extract value text data financial domain. Financial sector accumulates large amount financial communication text. recent machine learning breakthrough general Natural Language Understanding (NLU) language model BERT (Devlin et al. 2018) makes unstructured text data become goldmine generate value quantitative finance. Due unique data source, NLP-based factors tend merit orthogonal existing fundamental factors less multicollinearity issue. thus pivotal note NLP-based factors actually complement existing factors, instead disrupting overall. give detailed introduction Financial NLP latter sections, well deserve chapter right.another vein, much efforts focused employing modern high-dimensional machine learning techniques better model asset returns now given affluence data source. Applying machine learning routines financial data implicitly motivated AFA (American Finance Association) presidential address Cochrane (2011), suggests presence vast collection noisy highly correlated return predictors, need methods beyond cross-sectional linear regressions. Indeed, machine learning offers natural theoretical-sound way accommodate high-dimensional predictor set flexible functional forms, employs “regularization” methods select models, mitigate overfitting biases, uncover complex patterns hidden relationships.emerging body pioneer academic work reports phenomenal investment profitability based signals generated machine learning methods, let alone long data science enthusiasm investment industry. funny observed phenomenon echo trend nowadays Machine Learning PhDs likely land researcher job quantitative Hedge Funds pure Finance PhDs.list influential papers 3.1 roadmap recent modern factor investing. predictors column refer input features research use predict stock returns, Firm stands typical firm-level factors Macro represents macro indices. forthcoming sections deep dive representative ones (e.g., IPCA) alongside introducing corresponding machine learning algorithm.Table 3.1: Factor machine learning papers, till December 2021","code":""},{"path":"modern-factor-investing.html","id":"chapter-conclusion","chapter":"3 Modern Factor Investing","heading":"3.5 Chapter Conclusion","text":"Given plethora factors inevitable data mining, many historically discovered factors deemed “significant” chance.","code":""},{"path":"applications.html","id":"applications","chapter":"4 Applications","heading":"4 Applications","text":"significant applications demonstrated chapter.","code":""},{"path":"applications.html","id":"example-one","chapter":"4 Applications","heading":"4.1 Example one","text":"","code":""},{"path":"applications.html","id":"example-two","chapter":"4 Applications","heading":"4.2 Example two","text":"","code":""},{"path":"final-words.html","id":"final-words","chapter":"5 Final Words","heading":"5 Final Words","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
