[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Welcome first edition Machine Learning Quantitative Investment (ml4quant)! online book machine learning data science methods quantitative investment presented Fintelligence Academy: https://fintelligence-academy.github.io/.adoption Machine Learning Artificial Intelligence continues progress niche activity mainstream applications ever-accelerating pace Big Data era. Fintelligence, embracing ML financial application.book, aim walk ….quantitative research mind, intended audience presumed interdisciplinary, fluent mathematical notation, familiar basic data science concepts. may surprised neglect econometric/finance elements - confident teach foundational knowledge financial market along reading journey. addition, book also interesting readers thinking joining quant workforce data science community.another vein, book also brings mathematical foundations basic machine learning concepts fore collects information single place reader learns modern quantitative finance machine learning. assimilate benefit core ideas two classical machine learning textbooks Introduction Statistical Learning (ISL2) Elements Statistical Learning (ESL), best -go reference books learn machine learning. ml4quant, strive strike balance difficulty middle level two books season topic new flavor financial data science.genuinely hope quantitative researchers data scientists enjoy reading interdisciplinary book find helpful.“best time plant tree twenty years ago. second best time now.” - Chinese Proverb Poet Wang Bo“东隅已逝，桑榆非晚。”-王渤《滕王阁序》","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Machine learning latest long line attempts distill human knowledge reasoning form suitable constructing computer automated systems. enthusiastic practitioner interested learn magic behind successful machine learning algorithms currently faces daunting set pre-requisite knowledge:Programming languages data analysis toolsProgramming languages data analysis toolsLarge-scale computation associated frameworksLarge-scale computation associated frameworksMathematics statistics machine learning builds itMathematics statistics machine learning builds itMachine learning builds upon language mathematics express concepts seem intuitively obvious surprisingly difficult formalize. formalized properly, can gain insights task want solve.can label chapter section titles using {#label} , e.g., can reference Chapter 1. manually label , automatic labels anyway, e.g., Chapter 3.Figures tables captions placed figure table environments, respectively.\nFigure 1.1: nice figure!\nReference figure code chunk label fig: prefix, e.g., see Figure 1.1. Similarly, can reference tables generated knitr::kable(), e.g., see Table 1.1.Table 1.1: nice table!can write citations, . example, using bookdown package (Xie 2021) sample book, built top R Markdown knitr (Xie 2015).","code":"\npar(mar = c(4, 4, .1, .1))\nplot(pressure, type = 'b', pch = 19)\nknitr::kable(\n  head(iris, 20), caption = 'Here is a nice table!',\n  booktabs = TRUE\n)"},{"path":"elements-of-machine-learning.html","id":"elements-of-machine-learning","chapter":"2 Elements of Machine Learning","heading":"2 Elements of Machine Learning","text":"minimalism style, machine learning refers computers can “learn” finding patterns data using make predictions. Mathematically, given real-valued output \\(y\\) predictors vector \\(X\\) containing \\(p\\) variables, assume general function \\(f\\) describe \\(y\\):\\[\\begin{equation} \ny = f(X) + \\epsilon\n\\tag{2.1}\n\\end{equation}\\], \\(f\\) fixed unknown function, \\(\\epsilon\\) zero-mean random error term, supposed independent \\(X\\).core machine learning suite data-driven algorithms estimating \\(f\\). ML based statistical learning theory design models understand patterns employs optimization algorithms train model “learn” pattern using input data.foundation practical machine learning data. Data drives everything else. model can learn much pattern without enough data even biased behaviour data quality poor. contrast, substantial data, machine learning model achieve impressive results beyond expectation. vivid example check google’s search engine - machine learning algorithm hood.“models wrong, useful.” - George E.P. Box","code":""},{"path":"elements-of-machine-learning.html","id":"linear-regression","chapter":"2 Elements of Machine Learning","heading":"2.1 Linear Regression","text":"Linear regression adopts linear function\\(f^{Linear}\\) equation (2.1) learnable model parameters \\(\\beta^T =(\\beta_0,\\beta_1,...,\\beta_p) \\(p,1)\\):\\[\\begin{equation} \ny = \\beta^T X + \\epsilon  \n\\tag{2.2}\n\\end{equation}\\]assume first column \\(X\\) ones “intercept feature” thus \\(\\beta_0\\) corresponds intercept term.model given (2.2) defines population regression line, best linear approximation true relationship \\(X\\) \\(u\\). population regression line unobserved access set sample observations compute sample-based estimate line. Fundamentally, apply standard statistical approach using information sample estimate characteristics large population. linear regression focus estimate coefficient \\(\\hat{\\beta}\\).popular estimation method least squares, also known Ordinary Least Squares (OLS). OLS regression estimated model based sample data pick coefficients \\(\\hat{\\beta}\\) minimize residual sum squares (RSS):\\[\\begin{equation} \nRSS(\\beta) = \\sum_{=1}^N (y_i - f(x_i))^2 = (y-X\\beta)^T (y-X \\beta)  \n\\tag{2.3}\n\\end{equation}\\]minimize (2.3), differentiate term respect \\(\\beta\\) obtain:\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2X^T (y - X \\beta) \\\\\n\\frac{\\partial RSS}{\\partial \\beta \\partial \\beta^T} =  2 X^TX\n\\]Assuming \\(X\\) full column rank, hence \\(X^TX\\) positive definite leading optimality, thus set first derivative zero get \\(\\hat{\\beta}\\):\\[\nX^T (y - X \\beta) = 0  \\\\\n\\Rightarrow  X^T y  =  X^T X \\hat{\\beta}\n\\]thus solve unique solution:\\[\\begin{equation} \n\\hat{\\beta} = (X^TX)^{-1} X^T y\n\\tag{2.4}\n\\end{equation}\\]Please note derivation makes assumptions validity model - simply finds best linear fit data except ensuring \\(X\\) full-rank. non-full-rank case occurs often redundancy. natural way resolve non-unique representation drop redundant columns \\(X\\).Lastly, since \\(\\hat{y}=X \\hat{\\beta}\\), form predictions\n\\[\\begin{equation} \n\\hat{y} = X \\hat{\\beta} = (X(X^TX)^{-1} X^T) y\n\\tag{2.5}\n\\end{equation}\\]denote \\(P=X(X^TX)^{-1} X^T\\) projection matrix. Moreover, represent residual \\(\\hat{\\epsilon}\\) \\(P\\):\n\\[\\begin{equation} \n\\hat{\\epsilon} = y - X \\hat{\\beta} = y - (X(X^TX)^{-1} X^T) y = (I_n - P) y\n\\tag{2.6}\n\\end{equation}\\]Note \\(Q=I_n - P\\) also projection matrix. nice properties projection matrices use subsequent sections:\n\\[\nQ^T=Q; \\;Q^2=Q\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"sampling-uncertainty-and-statistical-inference","chapter":"2 Elements of Machine Learning","heading":"2.1.1 Sampling Uncertainty and Statistical Inference","text":"infer (2.4) \\(\\hat{\\beta}\\) \\(\\hat{Y}= X \\hat{\\beta}\\) linear transformations \\(y\\) random variables. collected different sample data size \\(n\\), sure different estimates, due sampling uncertainty. want quantify uncertainty using sampling statistics essentially get control estimate coefficient \\(\\hat{\\beta}\\). Furthermore, inclusion uncertainty measure standard error enables us conduct statistical inference.pin sampling properties \\(\\hat{\\beta}\\), include supplementary assumptions including uncorrelated residual, constant variance residual \\(\\sigma^2\\), additionally \\(X\\) non-random. Based assumption, infer variance \\(y\\):\\[\\begin{equation} \nVar(y)=\\sigma^2 I_n\n\\tag{2.7}\n\\end{equation}\\]derive expectation residual sum squares (RSS):\\[\nRSS = Y^T(1-P)Y  \\\\\n\\Rightarrow  E[ Y^T(1-P)Y ] = (n-p) \\sigma^2\n\\]Hence get unbiased estimate variance \\(\\hat{\\sigma}^2 = \\frac{RSS}{n-p}\\). next derive variance-covariance matrix \\(\\hat{\\beta}\\) using equation (2.7):\n\\[\n\\begin{aligned}\nVar( \\hat{\\beta}) &= Var( X(X^TX)^{-1} X^T ) \\\\&= X(X^TX)^{-1} X^T Var(y) (X(X^TX)^{-1} X^T)^T \\\\&=\\sigma^2 (X^TX)^{-1}\n\\end{aligned}\n\\]\nTherefore, uncertainty individual estimate \\(\\beta_j\\) quantified standard error (\\(SE\\))\n\\[\n\\begin{aligned}\nSE(\\beta_j) = \\sqrt{ \\frac{\\sigma}{ (X^TX)^{-1}_{jj} }}\n\\end{aligned}\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"rethinking-the-unbiasedness-of-ols","chapter":"2 Elements of Machine Learning","heading":"2.1.2 Rethinking the Unbiasedness of OLS","text":"require property unbiasedness estimator \\(\\hat{B}\\). Unbiased estimator systematically - -estimate true parameter \\(B\\).Consider decomposition mean squared error estimator \\(\\tilde{\\theta}\\) estimating \\(\\theta\\):\\[\n\\begin{aligned}\nMSE(\\tilde{\\theta}) &= E[ \\tilde{\\theta} - \\theta ]^2 \\\\\n                    &= Var( \\tilde{\\theta} ) +  [E( \\tilde{\\theta} ) - \\theta ]^2\n\\end{aligned}\n\\]first term \\(Var( \\tilde{\\theta} )\\) variance, second term \\([E( \\tilde{\\theta} ) - \\theta ]^2\\) squared bias.Gauss-Markov (GM) theorem implies least squares estimator smallest mean squared error (MSE) linear estimators bias (⚠️ pay attention “bias” ). However, may well exist biased estimator smaller mean squared error. estimator trade little bias larger reduction variance. Put bluntly, biased estimates commonly used better --sample performance (.e., less MSE test set). reality, method shrinks sets zero least squares coefficients may result biased estimate.","code":""},{"path":"elements-of-machine-learning.html","id":"assumptions-of-linear-regression-and-violation-implications","chapter":"2 Elements of Machine Learning","heading":"2.2 Assumptions of Linear Regression and Violation Implications","text":"question assumption linear regression ill-posed requires context need specify desired properties want linear model hold. Hence, start standard assumptions guarantee GM theorem hold OLS regression estimate:\n1. exists additive linear model \\((X, Y)\\) equation (2.1),\n\\[\nY= X \\beta + \\epsilon\n\\]\n, \\(\\beta\\) \\(X\\) non-random randomness stem \\(\\epsilon\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Furthermore, error terms 5. normally distributed 6.identically independently distributed (..d.), infer OLS estimator becomes Maximum Likelihood Estimation (MLE).","code":""},{"path":"elements-of-machine-learning.html","id":"problem-1-non-linearity-of-the-data","chapter":"2 Elements of Machine Learning","heading":"Problem 1: Non-linearity of the Data","text":"linear regression model assumes straight-line relationship predictors response. true relationship far linear, conclusions draw fit suspect.🔍 Residual plots useful visualization tool identifying non-linearity. Given fitted linear regression model, can plot residuals, \\(e_i = y_i - \\hat{y_i}\\), versus predictor \\(x_i\\). case multivariate regression model, can plot residuals versus fitted values \\(\\hat{y_i}\\). Independence assumption implies discernible pattern. presence pattern may indicate problem aspect linear model. instance, residuals exhibit clear U-shape, provides strong indication non-linearity data.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-2-correlation-of-error-terms","chapter":"2 Elements of Machine Learning","heading":"Problem 2: Correlation of Error Terms","text":"important assumption linear regression model error terms \\({\\epsilon_i}\\) uncorrelated. Intuitively, errors uncorrelated, fact \\(epsilon_i\\) positive provides little information sign \\(\\epsilon_{+1}\\). One counterexample think double set training observations \\((X, y)\\), \\(X_{Copy} = [X;X], y_{Copy}=[y;y]\\).standard errors computed estimated regression coefficients fitted values based assumption uncorrelated error terms. fact correlation among error terms, estimated standard errors tend underestimate true standard errors. result, confidence prediction intervals narrower expected.residual correlations frequently occur context time series data, consists observations measurements obtained discrete points time. issue residual autocorrelation - tracking residuals adjacent residuals may similar values.general, assumption uncorrelated errors extremely important linear regression well statistical methods, good experimental design crucial order mitigate risk correlations.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-3-outliers","chapter":"2 Elements of Machine Learning","heading":"Problem 3: Outliers","text":"outlier point far value predicted model. Outliers can arise variety reasons, incorrect data collection. quite common real dataset.Residual plots can used identify outliers. believe outlier occurred due error data collection, one solution simply remove observation. Shrewd care revised assumptions taken removing outliers, since outlier may instead indicate deficiency model, missing predictor. 2008 global financial crisis data point needs justification discarded outlier.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-4-collinearity","chapter":"2 Elements of Machine Learning","heading":"Problem 4: 💥Collinearity","text":"Collinearity refers situation group variables closely related one another. presence collinearity can pose problems regression context, since can difficult separate individual effects collinear variables response. Collinearity reduces accuracy estimates regression coefficients, causes standard error \\(\\hat{\\beta_j}\\) grow.🔍 Correlation matrix predictors simple way detect collinearity. Unfortunately, collinearity problems can detected inspection correlation matrix: possible collinearity exist three variables even pair variables particularly high correlation - called multicollinearity.two practical solutions problem collinearity. first, straightforward one, drop one problematic variables regression. second solution aggregate collinear variables together single composite predictor. example, apply PCA (Principal Component Analysis) approach model highly correlated features group extract first PC component representative indicator.","code":""},{"path":"elements-of-machine-learning.html","id":"conclusion","chapter":"2 Elements of Machine Learning","heading":"2.3 Conclusion","text":"Linear models largely developed precomputer age statistics, still robust baseline model apply today’s computer era. simple often provide adequate interpretable description inputs affect output. prediction purposes can sometimes outperform fancier nonlinear models, especially situations small numbers training cases, low signal--noise ratio sparse data.","code":""},{"path":"methods.html","id":"methods","chapter":"3 Methods","heading":"3 Methods","text":"describe methods chapter.Math can added body using usual syntax like ","code":""},{"path":"methods.html","id":"math-example","chapter":"3 Methods","heading":"3.1 math example","text":"\\(p\\) unknown expected around 1/3. Standard error approximated\\[\nSE = \\sqrt(\\frac{p(1-p)}{n}) \\approx \\sqrt{\\frac{1/3 (1 - 1/3)} {300}} = 0.027\n\\]can also use math footnotes like this1.approximate standard error 0.0272","code":""},{"path":"applications.html","id":"applications","chapter":"4 Applications","heading":"4 Applications","text":"significant applications demonstrated chapter.","code":""},{"path":"applications.html","id":"example-one","chapter":"4 Applications","heading":"4.1 Example one","text":"","code":""},{"path":"applications.html","id":"example-two","chapter":"4 Applications","heading":"4.2 Example two","text":"","code":""},{"path":"final-words.html","id":"final-words","chapter":"5 Final Words","heading":"5 Final Words","text":"finished nice book.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
