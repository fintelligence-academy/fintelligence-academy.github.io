[{"path":"index.html","id":"preface","chapter":"Preface","heading":"Preface","text":"Welcome first edition Machine Learning Quantitative Investment (ml4quant)! online book machine learning data science methods quantitative investment presented Fintelligence Academy: https://fintelligence-academy.github.io/.adoption Machine Learning Artificial Intelligence continues progress niche activity mainstream applications ever-accelerating pace Big Data era. Fintelligence, embracing ML financial application.book, aim walk ‚Ä¶.quantitative research mind, intended audience presumed interdisciplinary, fluent mathematical notation, familiar basic data science concepts. may surprised neglect econometric/finance elements - confident teach foundational knowledge financial market along reading journey. addition, book also interesting readers thinking joining quant workforce data science community.another vein, book also brings mathematical foundations basic machine learning concepts fore collects information single place reader learns modern quantitative finance machine learning. assimilate benefit core ideas two classical machine learning textbooks Introduction Statistical Learning (ISL2) Elements Statistical Learning (ESL), best -go reference books learn machine learning. ml4quant, strive strike balance difficulty middle level two books season topic new flavor financial data science.genuinely hope quantitative researchers data scientists enjoy reading interdisciplinary book find helpful.‚Äúbest time plant tree twenty years ago. second best time now.‚Äù - Chinese Proverb Poet Wang Bo‚Äú‰∏úÈöÖÂ∑≤ÈÄùÔºåÊ°ëÊ¶ÜÈùûÊôö„ÄÇ‚Äù-ÁéãÊ∏§„ÄäÊªïÁéãÈòÅÂ∫è„Äã","code":""},{"path":"intro.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Already, artificial intelligence around us, self-driving cars virtual assistants software translate invest. Impressive progress made AI recent years, driven exponential increases computing power availability vast amounts data, software used discover new drugs recommendation algorithms used predict cultural interests.Machine learning latest long line attempts distill human knowledge reasoning form suitable constructing computer automated systems. enthusiastic practitioner interested learn magic behind successful machine learning algorithms currently faces daunting set pre-requisite knowledge:Programming languages data analysis toolsProgramming languages data analysis toolsLarge-scale computation associated frameworksLarge-scale computation associated frameworksMathematics statistics machine learning builds itMathematics statistics machine learning builds itMachine learning builds upon language mathematics express concepts seem intuitively obvious surprisingly difficult formalize. formalized properly, can gain insights task want solve.underlying theme acceleration innovation velocity disruption hard comprehend anticipate drivers constitute source constant surprise, even best connected well informed. Indeed, across industries, clear evidence technologies underpin Fourth Industrial Revolution major impact businesses.collaborative innovationand relentlessly continuously innovate.","code":""},{"path":"elements-of-machine-learning.html","id":"elements-of-machine-learning","chapter":"2 Elements of Machine Learning","heading":"2 Elements of Machine Learning","text":"minimalism style, machine learning refers computers can ‚Äúlearn‚Äù finding patterns data using make predictions. Mathematically, given real-valued output \\(y\\) predictors vector \\(X\\) containing \\(p\\) variables, assume general function \\(f\\) describe \\(y\\):\\[\\begin{equation} \ny = f(X) + \\epsilon\n\\tag{2.1}\n\\end{equation}\\], \\(f\\) fixed unknown function, \\(\\epsilon\\) zero-mean random error term, supposed independent \\(X\\).core machine learning suite data-driven algorithms estimating \\(f\\). ML based statistical learning theory design models understand patterns employs optimization algorithms train model ‚Äúlearn‚Äù pattern using input data.foundation practical machine learning data. Data drives everything else. model can learn much pattern without enough data even biased behaviour data quality poor. contrast, substantial data, machine learning model achieve impressive results beyond expectation. vivid example check google‚Äôs search engine - machine learning algorithm hood.‚Äúmodels wrong, useful.‚Äù - George E.P. Box","code":""},{"path":"elements-of-machine-learning.html","id":"linear-regression","chapter":"2 Elements of Machine Learning","heading":"2.1 Linear Regression","text":"Linear regression adopts linear function\\(f^{Linear}\\) equation (2.1) learnable model parameters \\(\\beta^T =(\\beta_0,\\beta_1,...,\\beta_p) \\(p,1)\\):\\[\\begin{equation} \ny = \\beta^T X + \\epsilon  \n\\tag{2.2}\n\\end{equation}\\]assume first column \\(X\\) ones ‚Äúintercept feature‚Äù thus \\(\\beta_0\\) corresponds intercept term.model given (2.2) defines population regression line, best linear approximation true relationship \\(X\\) \\(u\\). population regression line unobserved access set sample observations compute sample-based estimate line. Fundamentally, apply standard statistical approach using information sample estimate characteristics large population. linear regression focus estimate coefficient \\(\\hat{\\beta}\\).popular estimation method least squares, also known Ordinary Least Squares (OLS). OLS regression estimated model based sample data pick coefficients \\(\\hat{\\beta}\\) minimize residual sum squares (RSS):\\[\\begin{equation} \nRSS(\\beta) = \\sum_{=1}^N (y_i - f(x_i))^2 = (y-X\\beta)^T (y-X \\beta)  \n\\tag{2.3}\n\\end{equation}\\]minimize (2.3), differentiate term respect \\(\\beta\\) obtain:\\[\n\\frac{\\partial RSS}{\\partial \\beta} = -2X^T (y - X \\beta) \\\\\n\\frac{\\partial RSS}{\\partial \\beta \\partial \\beta^T} =  2 X^TX\n\\]Assuming \\(X\\) full column rank, hence \\(X^TX\\) positive definite leading optimality, thus set first derivative zero get \\(\\hat{\\beta}\\):\\[\nX^T (y - X \\beta) = 0  \\\\\n\\Rightarrow  X^T y  =  X^T X \\hat{\\beta}\n\\]thus solve unique solution:\\[\\begin{equation} \n\\hat{\\beta} = (X^TX)^{-1} X^T y\n\\tag{2.4}\n\\end{equation}\\]Please note derivation makes assumptions validity model - simply finds best linear fit data except ensuring \\(X\\) full-rank. non-full-rank case occurs often redundancy. natural way resolve non-unique representation drop redundant columns \\(X\\).Lastly, since \\(\\hat{y}=X \\hat{\\beta}\\), form predictions\n\\[\\begin{equation} \n\\hat{y} = X \\hat{\\beta} = (X(X^TX)^{-1} X^T) y\n\\tag{2.5}\n\\end{equation}\\]denote \\(P=X(X^TX)^{-1} X^T\\) projection matrix. Moreover, represent residual \\(\\hat{\\epsilon}\\) \\(P\\):\n\\[\\begin{equation} \n\\hat{\\epsilon} = y - X \\hat{\\beta} = y - (X(X^TX)^{-1} X^T) y = (I_n - P) y\n\\tag{2.6}\n\\end{equation}\\]Note \\(Q=I_n - P\\) also projection matrix. nice properties projection matrices use subsequent sections:\n\\[\nQ^T=Q; \\;Q^2=Q\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"sampling-uncertainty-and-statistical-inference","chapter":"2 Elements of Machine Learning","heading":"2.1.1 Sampling Uncertainty and Statistical Inference","text":"infer (2.4) \\(\\hat{\\beta}\\) \\(\\hat{Y}= X \\hat{\\beta}\\) linear transformations \\(y\\) random variables. collected different sample data size \\(n\\), sure different estimates, due sampling uncertainty. want quantify uncertainty using sampling statistics essentially get control estimate coefficient \\(\\hat{\\beta}\\). Furthermore, inclusion uncertainty measure standard error enables us conduct statistical inference.pin sampling properties \\(\\hat{\\beta}\\), include supplementary assumptions including uncorrelated residual, constant variance residual \\(\\sigma^2\\), additionally \\(X\\) non-random. Based assumption, infer variance \\(y\\):\\[\\begin{equation} \nVar(y)=\\sigma^2 I_n\n\\tag{2.7}\n\\end{equation}\\]derive expectation residual sum squares (RSS):\\[\nRSS = Y^T(1-P)Y  \\\\\n\\Rightarrow  E[ Y^T(1-P)Y ] = (n-p) \\sigma^2\n\\]Hence get unbiased estimate variance \\(\\hat{\\sigma}^2 = \\frac{RSS}{n-p}\\). next derive variance-covariance matrix \\(\\hat{\\beta}\\) using equation (2.7):\n\\[\n\\begin{aligned}\nVar( \\hat{\\beta}) &= Var( X(X^TX)^{-1} X^T ) \\\\&= X(X^TX)^{-1} X^T Var(y) (X(X^TX)^{-1} X^T)^T \\\\&=\\sigma^2 (X^TX)^{-1}\n\\end{aligned}\n\\]\nTherefore, uncertainty individual estimate \\(\\beta_j\\) quantified standard error (\\(SE\\))\n\\[\n\\begin{aligned}\nSE(\\beta_j) = \\sqrt{ \\frac{\\sigma}{ (X^TX)^{-1}_{jj} }}\n\\end{aligned}\n\\]","code":""},{"path":"elements-of-machine-learning.html","id":"rethinking-the-unbiasedness-of-ols","chapter":"2 Elements of Machine Learning","heading":"2.1.2 Rethinking the Unbiasedness of OLS","text":"require property unbiasedness estimator \\(\\hat{B}\\). Unbiased estimator systematically - -estimate true parameter \\(B\\).Consider decomposition mean squared error estimator \\(\\tilde{\\theta}\\) estimating \\(\\theta\\):\\[\n\\begin{aligned}\nMSE(\\tilde{\\theta}) &= E[ \\tilde{\\theta} - \\theta ]^2 \\\\\n                    &= Var( \\tilde{\\theta} ) +  [E( \\tilde{\\theta} ) - \\theta ]^2\n\\end{aligned}\n\\]first term \\(Var( \\tilde{\\theta} )\\) variance, second term \\([E( \\tilde{\\theta} ) - \\theta ]^2\\) squared bias.Gauss-Markov (GM) theorem implies least squares estimator smallest mean squared error (MSE) linear estimators bias (‚ö†Ô∏è pay attention ‚Äúbias‚Äù ). However, may well exist biased estimator smaller mean squared error. estimator trade little bias larger reduction variance. Put bluntly, biased estimates commonly used better --sample performance (.e., less MSE test set). reality, method shrinks sets zero least squares coefficients may result biased estimate.","code":""},{"path":"elements-of-machine-learning.html","id":"assumptions-of-linear-regression-and-violation-implications","chapter":"2 Elements of Machine Learning","heading":"2.2 Assumptions of Linear Regression and Violation Implications","text":"question assumption linear regression ill-posed requires context need specify desired properties want linear model hold. Hence, start standard assumptions guarantee GM theorem hold OLS regression estimate:\n1. exists additive linear model \\((X, Y)\\) equation (2.1),\n\\[\nY= X \\beta + \\epsilon\n\\]\n, \\(\\beta\\) \\(X\\) non-random randomness stem \\(\\epsilon\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Zero Expected Mean Residual: \\(E[ \\epsilon_i] = 0\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Homoscedasticity: Constant Variance Residual: \\(Var(\\epsilon_i)=\\sigma^2 < \\infty\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Uncorrelated Residual Error: \\(Cov(\\epsilon_i, \\epsilon_j)=0, \\; \\forall \\neq j\\)Furthermore, error terms 5. normally distributed 6.identically independently distributed (..d.), infer OLS estimator becomes Maximum Likelihood Estimation (MLE).","code":""},{"path":"elements-of-machine-learning.html","id":"problem-1-non-linearity-of-the-data","chapter":"2 Elements of Machine Learning","heading":"Problem 1: Non-linearity of the Data","text":"linear regression model assumes straight-line relationship predictors response. true relationship far linear, conclusions draw fit suspect.üîç Residual plots useful visualization tool identifying non-linearity. Given fitted linear regression model, can plot residuals, \\(e_i = y_i - \\hat{y_i}\\), versus predictor \\(x_i\\). case multivariate regression model, can plot residuals versus fitted values \\(\\hat{y_i}\\). Independence assumption implies discernible pattern. presence pattern may indicate problem aspect linear model. instance, residuals exhibit clear U-shape, provides strong indication non-linearity data.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-2-correlation-of-error-terms","chapter":"2 Elements of Machine Learning","heading":"Problem 2: Correlation of Error Terms","text":"important assumption linear regression model error terms \\({\\epsilon_i}\\) uncorrelated. Intuitively, errors uncorrelated, fact \\(epsilon_i\\) positive provides little information sign \\(\\epsilon_{+1}\\). One counterexample think double set training observations \\((X, y)\\), \\(X_{Copy} = [X;X], y_{Copy}=[y;y]\\).standard errors computed estimated regression coefficients fitted values based assumption uncorrelated error terms. fact correlation among error terms, estimated standard errors tend underestimate true standard errors. result, confidence prediction intervals narrower expected.residual correlations frequently occur context time series data, consists observations measurements obtained discrete points time. issue residual autocorrelation - tracking residuals adjacent residuals may similar values.general, assumption uncorrelated errors extremely important linear regression well statistical methods, good experimental design crucial order mitigate risk correlations.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-3-outliers","chapter":"2 Elements of Machine Learning","heading":"Problem 3: Outliers","text":"outlier point far value predicted model. Outliers can arise variety reasons, incorrect data collection. quite common real dataset.Residual plots can used identify outliers. believe outlier occurred due error data collection, one solution simply remove observation. Shrewd care revised assumptions taken removing outliers, since outlier may instead indicate deficiency model, missing predictor. 2008 global financial crisis data point needs justification discarded outlier.","code":""},{"path":"elements-of-machine-learning.html","id":"problem-4-collinearity","chapter":"2 Elements of Machine Learning","heading":"Problem 4: üí•Collinearity","text":"Collinearity refers situation group variables closely related one another. presence collinearity can pose problems regression context, since can difficult separate individual effects collinear variables response. Collinearity reduces accuracy estimates regression coefficients, causes standard error \\(\\hat{\\beta_j}\\) grow.üîç Correlation matrix predictors simple way detect collinearity. Unfortunately, collinearity problems can detected inspection correlation matrix: possible collinearity exist three variables even pair variables particularly high correlation - called multicollinearity.two practical solutions problem collinearity. first, straightforward one, drop one problematic variables regression. second solution aggregate collinear variables together single composite predictor. example, apply PCA (Principal Component Analysis) approach model highly correlated features group extract first PC component representative indicator.","code":""},{"path":"elements-of-machine-learning.html","id":"conclusion","chapter":"2 Elements of Machine Learning","heading":"2.3 Conclusion","text":"Linear models largely developed precomputer age statistics, still robust baseline model apply today‚Äôs computer era. simple often provide adequate interpretable description inputs affect output. prediction purposes can sometimes outperform fancier nonlinear models, especially situations small numbers training cases, low signal--noise ratio sparse data.","code":""},{"path":"modern-factor-investing.html","id":"modern-factor-investing","chapter":"3 Modern Factor Investing","heading":"3 Modern Factor Investing","text":"give overview modern factor investing chapter. intended introductory chapter help readers get familiar quantitative finance framework. chapter adapted Webinar series Modern Factor Investing: Past, Present, Future.last 50 years, academic researchers made major breakthroughs advancing classical practice finance, fundamental analysis scientific quantitative research. include portfolio theory, corporate finance, financial engineering derivative instruments, many applications pertaining financial markets overall. One foundational pillar advances capital market theory context descriptive equilibrium propositions terms risk/return tradeoff linear factor model explain expected asset price, beginning William F. Sharpe Capital Asset Pricing Model (CAPM) (Sharpe 1964). Many related academic developments provided rich asset pricing insight, including Arbitrage Pricing Theory (APT) (Ross 1976), market efficiency proposition, market anomalies Fama French factor model (Fama French 1993), behavioral finance. altogether gave birth new investment paradigm - factor investing widely adopted finance industry - classical finance textbooks usually end .saying, ‚Äúchange constant life‚Äù attributed Greek philosopher, Heraclitus around 500BC. even prominent capital market. evolution financial market keeps testing efficacy factor theory model. Ever since 2008 global financial crisis, observe seemingly irreversible performance decay traditional factor model solely based conventional data linear framework. academia, also replication crisis (credibility crisis) raging ‚Äúclaimed research findings financial economics likely false.‚Äù (Harvey, Liu, Zhu 2016)Perhaps Fourth Industrial Revolution - digitalization - already come, ‚ÄúAI new electricity‚Äù. trend, factor models strategies remained static. Factor models investment strategies evolve gradually time, reflecting evolution capital markets advances data science theories, data availability, investment practice. current innovation efforts factor investing focus exploiting new unstructured data sources - alternative data, applying new data science modeling techniques - machine learning - modern factor investing starts .","code":""},{"path":"modern-factor-investing.html","id":"overview-of-factor-investing","chapter":"3 Modern Factor Investing","heading":"3.1 Overview of Factor Investing","text":"One central themes assets priced rationally, variables related average returns, size book--market equity, must proxy sensitivity common (shared thus undiversifiable) risk factors returns ‚Äî Fama French (1993)question drives asset returns staple neoclassical finance remains holly grail. factor model emerged became foundation asset pricing theory 1960s (Lintner 1969; Mossin 1966; Sharpe 1964; Treynor 1961). general, Factor can defined underlying characteristic relating group assets (e.g., equity, bond) linearly significant explaining return risk.Factor investing refer general use factors investment process. trustworthily quantitative paradigm investment factor models renders return prediction signal, forecast expected asset risk, drive asset allocation decisions using tradable factor portfolios. Practically, factor-driven investment strategies smart beta products amount $800 billion AUM worldwide (Johnson 2018) ‚Äî yes, talking magnificent investment topic may affect everyone‚Äôs saving pension.recognized Fama French (1993), firm characteristics (factors) associated cross-section expected returns. Assuming individual asset return \\(r_{,t}\\), factor loadings \\(z_{,t}\\) factor return \\(f_t\\), factor model linearly specifies relationship:\\[\\begin{equation} \nr_{,t} = z'_{,t-1}  f_t + \\alpha_{,t} + u_{,t}\n\\tag{3.1}\n\\end{equation}\\], \\(u_{,t}\\) noise term expected zero return \\(\\alpha_{,t}\\) intercept term common OLS regression.Now mathematical formulation, delve granular categories factors:Beta factors ‚Äî factors corresponding \\(f_t\\) generally referred ‚Äúrisk premia factors‚Äù, reflected exposure sources systematic risk supposed earn persistent significant risk-compensated premium long periods (model assumptions work‚Ä¶).\npractice, called ‚Äúbeta factors‚Äù ‚Äústyle factors‚Äù denote drivers systematic return risk assets. Furthermore, usually associated systematic risk inherent market.\nrisk premium reward thought selling insurance products Mr.¬†Market. undertake risk receive positive reward average long-term, risk happen, Mr Market come coverage suffer realized loss well anticipated. believe game fair agree philosophy passive investing.Beta factors ‚Äî factors corresponding \\(f_t\\) generally referred ‚Äúrisk premia factors‚Äù, reflected exposure sources systematic risk supposed earn persistent significant risk-compensated premium long periods (model assumptions work‚Ä¶).practice, called ‚Äúbeta factors‚Äù ‚Äústyle factors‚Äù denote drivers systematic return risk assets. Furthermore, usually associated systematic risk inherent market.risk premium reward thought selling insurance products Mr.¬†Market. undertake risk receive positive reward average long-term, risk happen, Mr Market come coverage suffer realized loss well anticipated. believe game fair agree philosophy passive investing.Alpha factors ‚Äî factors fall \\(\\alpha_{,t}\\) term explain risk well (e.g., volatile) can included linear terms, yet earn persistent return time predictive value result \\(\\alpha_{,t} \\neq 0\\).\nAlpha factor model designed forecast excess return stocks. return distribution characterized expected return standard deviation, often expected return predicted alphas determines whether buy sell, overweight underweight, standard deviation driven betas determines size portfolio allocations hedge undesirable systematic risk - monologue hedge funds active investment.\nEfficient market theory forbids existence alpha academia call ‚Äúanomaly‚Äù, however, hedge fund active managers rely HFs seem live well. alpha factors often called ‚Äúsignal‚Äù practitioners distinguish beta risk premia factors.\nalpha signals often proprietary highly guarded, reflecting creativity well superior systems. important differentiator within investment firm.Alpha factors ‚Äî factors fall \\(\\alpha_{,t}\\) term explain risk well (e.g., volatile) can included linear terms, yet earn persistent return time predictive value result \\(\\alpha_{,t} \\neq 0\\).Alpha factor model designed forecast excess return stocks. return distribution characterized expected return standard deviation, often expected return predicted alphas determines whether buy sell, overweight underweight, standard deviation driven betas determines size portfolio allocations hedge undesirable systematic risk - monologue hedge funds active investment.Efficient market theory forbids existence alpha academia call ‚Äúanomaly‚Äù, however, hedge fund active managers rely HFs seem live well. alpha factors often called ‚Äúsignal‚Äù practitioners distinguish beta risk premia factors.alpha signals often proprietary highly guarded, reflecting creativity well superior systems. important differentiator within investment firm.Factors bedrock quantitative investment management served needs investors long time. equities, addition countries industries, accounting-based fundamental factors documented empirical research used extensively portfolio management include value, size, momentum, volatility, quality, yield, growth, liquidity. similar set common macro drivers identified used across asset classes, including equity, rates, credit, real assets.","code":""},{"path":"modern-factor-investing.html","id":"years-of-factor-investing-a-literature-review","chapter":"3 Modern Factor Investing","heading":"3.2 50 Years of Factor Investing: A Literature Review","text":"Factor research prevalent 50 years. oldest well-known model stock returns Capital Asset Pricing Model (CAPM), essence single-factor model market sole factor (see Sharpe (1964)). words, expected return stock viewed function beta factor loading market . won William Sharpe Nobel Prize.Later, Ross (1976) proposed Arbitrage pricing theory (APT) holds expected return financial asset can modeled function various macroeconomic factors theoretical market indexes, thus introducing ‚Äúmulti-factor models‚Äù. Moreover, unlike CAPM, APT theoretically validated number nature APT factors likely change time vary across markets. Hence, challenge shifts build empirically sound factor models.Beyond market factor, researchers generally look risk premium factors persistent time strong explanatory power broad range stocks. three main categories factors today: macroeconomic, statistical, fundamental.Macroeconomic factors include measures surprises inflation (Chen, Roll, Ross 1986)Macroeconomic factors include measures surprises inflation (Chen, Roll, Ross 1986)Statistical factor models identify factors factors pre-specified advance (Chamberlain 1983)Statistical factor models identify factors factors pre-specified advance (Chamberlain 1983)Fundamental factors mostly widely used credit Fama French (1993). put forward model explaining US equity market returns three fundamental actors: Market, Size, Value. ‚ÄúFama-French‚Äù model, today includes Carhart (1997) Momentum factor, become canon within finance literature. ‚Äì Fama also later got Nobel Prize.Fundamental factors mostly widely used credit Fama French (1993). put forward model explaining US equity market returns three fundamental actors: Market, Size, Value. ‚ÄúFama-French‚Äù model, today includes Carhart (1997) Momentum factor, become canon within finance literature. ‚Äì Fama also later got Nobel Prize.another vein, Rosenberg (1974) described importance stock fundamental traits explaining variation stock returns, leading creation multi-factor Barra risk models still dominant risk model equity investment.","code":""},{"path":"modern-factor-investing.html","id":"limitedness-of-classical-factor-model-and-the-factor-zoo","chapter":"3 Modern Factor Investing","heading":"3.3 Limitedness of Classical Factor Model and the Factor Zoo","text":"enduring success fundamental factors stems fact guided academic theory, , importantly, supported empirical evidence reflect investment practice, .e., make money. However, story factors turn horror fiction ever since 2008 Global Financial Crisis. well-acclaimed factors outperform past start underperform (Fama-French‚Äôs Value factor lacklustre, see Figure 3.1), also lurking black swan events causing extreme outliers model fail drastically. statistics, usually associated heavy-tailed distribution remove winsorize data points outliers fitting model. Nevertheless, reality, outliers may mean doomed .\nFigure 3.1: Recent performance decay Fama-French factor portfolios US market\nmake concrete example, lets remember David Viniar‚Äôs, CFO Goldman Sachs, famous 25 sigma Goldman‚Äôs flagship GEO hedge fund lost 27% 2007: ‚Äúseeing things 25-standard deviation moves, several days row.‚Äù. One commentator wryly noted:Viniar. comic. According Goldman‚Äôs mathematical models, August, Year Lord 2007, special month. Things happening supposed happen every 100,000 years. Either ‚Ä¶ Goldman‚Äôs models wrong (Bonner, 2007b).direct implication normality assumption linear models hold financial market, thereby significantly weakening statistical inference power quantitative fiance. p-value relevant test statistics thus interpreted extreme caution neglected altogether. empirically recommend latter.importantly, underlying linearity assumption unrealistic assumption witness great amount evidence non-linearity breaking fundamental assumption. One straightforward counterexample interaction effect one factor‚Äôs contribution asset‚Äôs expected return may rely factors. Furthermore, complex pattern certain factor exerts asset return risk. forget theoretical unbiasedness credit linear model entering quantitative finance.\nFigure 3.2: control factor production (Harvey, 2019)\npoints multiple testing problem: many factors tried, appear ‚Äúsignificant‚Äù purely chance, indicates peril data mining, may may intentional. data science perspective, also leads high-dimension nature modern factor investing: now ‚Äúzoo‚Äù available factor candidates traditional OLS linear regression incompetent.overlook merits linear model transparency interpretability. Given careful treatment enhacncement regularization, linear model remains competiteive baseline, especially set-insufficient data input. However, performance upper bound inflated current data science world, many alternative expressive machine learning algorithms available: linear model parsimonious underfitted compromise (think hundreds factors!) face high noise--signal ratio financial data.","code":""},{"path":"modern-factor-investing.html","id":"modern-factor-investing-in-the-wave-of-big-data-and-machine-learning","chapter":"3 Modern Factor Investing","heading":"3.4 Modern Factor Investing in the Wave of Big Data and Machine Learning","text":"Big Data? wide array information sensing devices well cloud service technique led exponential growth available big datasets. Starting handheld mobiles, cameras, microphones, Internets, digitalised map, Internet Things (IoT), LiDAR (Light Detection Ranging) sensors satellite imaging. amount data world collects experienced explosive growth. rapid development informative technologies paved way transition digital world.rapid advancement Internet social media sectors far reaching effects day day lives; also, finance researchers investors. Quantitative finance longer subclass finance arguably whole finance evolving data-driven scientific subject. comes fashionable term finance domain - alternative data.Alternative data data typically used researchers model builders can sourced kaleidoscope data sources ranging text, audio, image, graph, etc. data sets typically minimal aggregation processing making difficult access use. last several years, seen explosion availability new alternative data sources.\nFigure 3.3: Increasing number alternative data vendors (Man Institute, 2019)\nalternative data sets market data previously difficult access (e.g., analyst forecasts, insider transactions, options trading information, credit default swaps, hedge fund positions). data sets available numerical format can sourced commercial vendors.alternative data sets market data previously difficult access (e.g., analyst forecasts, insider transactions, options trading information, credit default swaps, hedge fund positions). data sets available numerical format can sourced commercial vendors.new data sets come new atypical formats (e.g., text, audio, video), can extracted internet sites social media. Investors may gain additional insights processing understanding information contained unstructured alternative data sets news reports, product reviews, employee reviews, job postings, regulatory filings, call transcripts, satellite images, etc.new data sets come new atypical formats (e.g., text, audio, video), can extracted internet sites social media. Investors may gain additional insights processing understanding information contained unstructured alternative data sets news reports, product reviews, employee reviews, job postings, regulatory filings, call transcripts, satellite images, etc.new formats require new modeling techniques powerful computers extract information. Innovations information technology cloud computing bespoke microprocessors provide computational firepower necessary apply machine learning computationally intensive data science techniques new unstructured alternative data sets.Natural language processing (NLP) one exciting frontiers bring alternative value factor investing framework. essence, now leverage deep-learning based NLP model extract value text data financial domain. Financial sector accumulates large amount financial communication text. recent machine learning breakthrough general Natural Language Understanding (NLU) language model BERT (Devlin et al. 2018) makes unstructured text data become goldmine generate value quantitative finance. Due unique data source, NLP-based factors tend merit orthogonal existing fundamental factors less multicollinearity issue. thus pivotal note NLP-based factors actually complement existing factors, instead disrupting overall. give detailed introduction Financial NLP latter sections, well deserve chapter right.another vein, much efforts focused employing modern high-dimensional machine learning techniques better model asset returns now given affluence data source. Applying machine learning routines financial data implicitly motivated AFA (American Finance Association) presidential address Cochrane (2011), suggests presence vast collection noisy highly correlated return predictors, need methods beyond cross-sectional linear regressions. Indeed, machine learning offers natural theoretical-sound way accommodate high-dimensional predictor set flexible functional forms, employs ‚Äúregularization‚Äù methods select models, mitigate overfitting biases, uncover complex patterns hidden relationships.emerging body pioneer academic work reports phenomenal investment profitability based signals generated machine learning methods, let alone long data science enthusiasm investment industry. funny observed phenomenon echo trend nowadays Machine Learning PhDs likely land researcher job quantitative Hedge Funds pure Finance PhDs.list influential machine learning factor investing papers table 3.1 roadmap recent modern factor investing. predictors column refer input features research use predict stock returns, Firm stands typical firm-level factors Macro represents macro indicator. forthcoming sections deep dive representative ones (e.g., IPCA Kelly, Pruitt, Su (2019)) alongside introducing corresponding machine learning algorithm.Table 3.1: Factor machine learning papers, till December 2021","code":""},{"path":"modern-factor-investing.html","id":"chapter-conclusion","chapter":"3 Modern Factor Investing","heading":"3.5 Chapter Conclusion","text":"Factor investing paradigm quantitative finance 50 years history linearly describing asset return risk using effective intuitive asset characteristics - factors. enjoys academic acclaim fundamentally adopted industry.recent performance decay conventional factors points limitedness classical linear factor model. Moreover, continued growing number investment factors proposed academics practitioners leads high-dimensional ‚Äúfactor zoo‚Äù current factor world traditional linear regression thus become incompetent.Current innovation efforts factor investing focus exploiting new unstructured data sources, applying new data science modeling techniques machine learning harness high-dimensional factors identify evolving complex market payoff pattern.‚ÄúChange constant financial market.‚Äù - Fintelligence","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
