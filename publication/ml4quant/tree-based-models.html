<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Chapter 4 Tree-based Models | Machine Learning for Quantitative Investment</title>
<meta name="author" content="Fintelligence">
<meta name="description" content="We give an overview of tree-based machine learning model in this chapter, which is currently the GOATüêê prediction model for tabular dataset. Neural networks dominate image recognition and language...">
<meta name="generator" content="bookdown 0.24 with bs4_book()">
<meta property="og:title" content="Chapter 4 Tree-based Models | Machine Learning for Quantitative Investment">
<meta property="og:type" content="book">
<meta property="og:image" content="/images/bg-quant.png">
<meta property="og:description" content="We give an overview of tree-based machine learning model in this chapter, which is currently the GOATüêê prediction model for tabular dataset. Neural networks dominate image recognition and language...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Chapter 4 Tree-based Models | Machine Learning for Quantitative Investment">
<meta name="twitter:description" content="We give an overview of tree-based machine learning model in this chapter, which is currently the GOATüêê prediction model for tabular dataset. Neural networks dominate image recognition and language...">
<meta name="twitter:image" content="/images/bg-quant.png">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/header-attrs-2.11/header-attrs.js"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.3.1/transition.js"></script><script src="libs/bs3compat-0.3.1/tabs.js"></script><script src="libs/bs3compat-0.3.1/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS -->
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Machine Learning for Quantitative Investment</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html">Preface</a></li>
<li><a class="" href="intro.html"><span class="header-section-number">1</span> Introduction</a></li>
<li><a class="" href="elements-of-machine-learning.html"><span class="header-section-number">2</span> Elements of Machine Learning</a></li>
<li><a class="" href="modern-factor-investing.html"><span class="header-section-number">3</span> Modern Factor Investing</a></li>
<li><a class="active" href="tree-based-models.html"><span class="header-section-number">4</span> Tree-based Models</a></li>
<li><a class="" href="references.html">References</a></li>
</ul>

        <div class="book-extra">
          
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="tree-based-models" class="section level1" number="4">
<h1>
<span class="header-section-number">4</span> Tree-based Models<a class="anchor" aria-label="anchor" href="#tree-based-models"><i class="fas fa-link"></i></a>
</h1>
<blockquote>
<p>We give an overview of tree-based machine learning model in this chapter, which is currently the <em>GOAT</em>üêê prediction model for tabular dataset. Neural networks dominate image recognition and language processing tasks, while tree-based methods üå≤ are often the method of choice for tabular data such as Kaggle competition. This chapter is cuurently working in progress.</p>
</blockquote>
<div id="introduction-to-machine-learning-tree" class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Introduction to Machine Learning Tree üå≤<a class="anchor" aria-label="anchor" href="#introduction-to-machine-learning-tree"><i class="fas fa-link"></i></a>
</h2>
<p>Tree-based methods partition the feature space into a set of rectangles, and then fit a simple model (like a constant) in each one. They are conceptually simple yet powerful.</p>
<p>Mathematically, given two predictors variables <span class="math inline">\(X_1\)</span>,<span class="math inline">\(X_2\)</span>, a tree model <span class="math inline">\(\hat{f}_{tree}\)</span> is formulated as a sequence of <span class="math inline">\(K\)</span> recursive binary partitions based on the feature <span class="math inline">\(X\)</span> to fit the target variable <span class="math inline">\(Y\)</span>. The corresponding regression model predicts <span class="math inline">\(Y\)</span> with a constant <span class="math inline">\(c_m\)</span> in rectangle region <span class="math inline">\(R_m\)</span> as:</p>
<p><span class="math display">\[
\hat{f}_{tree} (x) = \sum_{m=1}^K c_m I\{ (x_1, x_2) \in R_m \} 
\]</span></p>
<div class="inline-figure"><img src="images/chapter4/simple_tree.png" width="100%"></div>
<p>We now turn to the question of how to <span style="color:#00ea8d"><em>grow</em></span> a regression tree. The algorithm needs to automatically figure out the splitting variables, split points, as well as what topology (shape) the tree should have. Given a partition into <span class="math inline">\(K\)</span> regions <span class="math inline">\(R_1, R_2,...,R_K\)</span>, we could start from use a constant <span class="math inline">\(c_m\)</span> in each region to represent the shape:</p>
<p><span class="math display">\[
f_{tree} (X) = \sum_{m=1}^K c_m I\{ x \in R_m \} 
\]</span>
Adopting the sum of squares <span class="math inline">\(\sum(y_i - f_{tree}(x_i))^2\)</span> as objective to minimize, the best <span class="math inline">\(c_m\)</span> will be the average of <span class="math inline">\(y_i\)</span> within the region <span class="math inline">\(R_m\)</span>:
<span class="math display">\[
\hat{c}_m = avg(y_i | x_i \in R_m)
\]</span>
Finding the best binary partition in terms of minimum sum of squares is generally computationally infeasible. Hence we proceed with a greedy algorithm. Starting with all of the data, consider a splitting variable <span class="math inline">\(j\)</span> and split point <span class="math inline">\(s\)</span>, and define the pair of half-planes <span class="math inline">\(R_{L}(j,s)={X|X_j \leq s},\; R_{R}(j,s)={X|X_j &gt; s}\)</span>. We seek the splitting variable <span class="math inline">\(j\)</span> and split point <span class="math inline">\(s\)</span> that solves
<span class="math display">\[
\min_{j,s}  [ \sum_{x_i \in R_{L}(j,s)} (y_i - \hat{c}_L) + \sum_{x_i \in R_{R}(j,s)} (y_i - \hat{c}_R)]
\]</span>
Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. Then this process is repeated on all of the resulting regions until a stopping criterion is triggered.</p>
<p>How large should we grow the tree? A very large tree might overfit the data, while a small tree might not capture the important structure. Tree size is a tuning parameter governing the model‚Äôs complexity, and the optimal tree size should be adaptively chosen from the data. The preferred strategy is to grow a large tree <span class="math inline">\(T\)</span>, stopping the splitting process only when some minimum node size (e.g.¬†6 data point nodes) is reached. This large tree is next reversely (bottom-up) <span style="color:#00ea8d"><em>pruned</em></span> by using <em>cost-complexity pruning</em>.</p>
<p>Tree pruning ‚úÇÔ∏è refers to collapse any number of its internal (non-terminal) nodes from tree <span class="math inline">\(T_0\)</span> into a single subtree <span class="math inline">\(T \subset T_0\)</span>. Pruning a tree thus removes the split and helps to prevent overfitting the training data. We denote the number of terminal nodes in <span class="math inline">\(T\)</span> as <span class="math inline">\(|T|\)</span>, next we derive the cost complexity criterion with hyperparameter <span class="math inline">\(\alpha\)</span> to determine the <span class="math inline">\(T\)</span>:
<span class="math display">\[
C_\alpha (T) \ \sum_{m=1}^{|T|}  \sum_{x_i \in R_m}(y_i - \hat{c}_m)^2 + \alpha |T|
\]</span></p>
<p>The idea is to find a tree <span class="math inline">\(T_{\alpha}\)</span> that could minimize <span class="math inline">\(C_\alpha (T)\)</span>. Here the tuning <span class="math inline">\(\alpha\)</span> governs the tradeoff between tree size and its goodness of fit to the data. Small values of <span class="math inline">\(\alpha\)</span> result in larger trees <span class="math inline">\(T_{\alpha}\)</span>. With <span class="math inline">\(\alpha=0\)</span>, the solution is the full tree <span class="math inline">\(T_0\)</span>.</p>
</div>
<div id="boosting-and-additive-trees" class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Boosting and Additive Trees<a class="anchor" aria-label="anchor" href="#boosting-and-additive-trees"><i class="fas fa-link"></i></a>
</h2>
<p>Boosting is a predictive model paradigm developed for additive expansions by building models <span class="math inline">\(f_{(b)}(x)\)</span> sequentially. This involves starting with a null model and then repeatedly fitting a small model to the residuals of the current model and updating the current model with a down-weighted or ‚Äòshrunken‚Äô version of that weak models.</p>
<p><span class="math display" id="eq:boosted">\[\begin{equation} 
\hat{f}^{boost}(x) = \sum_{b=1}^B \beta^b   f_{(b)}(x; \theta_b)
\tag{4.1}
\end{equation}\]</span></p>
</div>
<div id="gradient-boosting-tree" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Gradient Boosting Tree<a class="anchor" aria-label="anchor" href="#gradient-boosting-tree"><i class="fas fa-link"></i></a>
</h2>
<p>The most popular variations of boosting are gradient boosting tree (GBT) and its modern variations (e.g.¬†LightGBM, XGBoost, CatBoost). Decision trees <span class="math inline">\(f_{(b)}^{tree}(x; \theta_b)\)</span> are put as standard building block in the gradient boosting tree. GBT grows the tree sequentially by iteratively fitting the <span class="math inline">\(f_{(b)}^{tree}(x)\)</span> through a gradient descent optimization manner with a shrinkage regularization parameter learning rate <span class="math inline">\(\lambda \in (0,1)\)</span> to avoid overfitting.</p>
<p><span class="math display" id="eq:gbtree">\[\begin{equation} 
\hat{f}^{GBT}(x) = \sum_{b=1}^B \lambda^b   f_{(b)}(x; \theta_b)
\tag{4.2}
\end{equation}\]</span></p>
<div id="algorithm" class="section level3" number="4.3.1">
<h3>
<span class="header-section-number">4.3.1</span> Algorithm<a class="anchor" aria-label="anchor" href="#algorithm"><i class="fas fa-link"></i></a>
</h3>
<p>We show the current standard algorithm to train a gradient boosting tree.</p>
<hr>
<p><strong>Algorithm</strong>: Gradient Boosting Tree</p>
<hr>
<p><strong>Input</strong>: sample predictors <span class="math inline">\(X\)</span>, target response <span class="math inline">\(y\)</span> of total data sample size <span class="math inline">\(N\)</span></p>
<p><strong>Parameters</strong>: <span class="math inline">\(B\)</span>: the number of boosting stages to perform, <span class="math inline">\(\lambda\)</span>: learning rate shrinks the contribution of each tree, <span class="math inline">\(\eta\)</span>: the fraction of subsamples to be used for fitting the individual base learner tree, <span class="math inline">\(d \in (1,2,...)\)</span>: the maximum depth of the individual regression estimators</p>
<ol style="list-style-type: decimal">
<li><p>Set <span class="math inline">\(r_0 = y\)</span> as initial point and fit a initial tree model <span class="math inline">\(f^{GBT}_{(0)}(x; d)\)</span> on sample training data <span class="math inline">\((X, r_0)\)</span></p></li>
<li>
<p>For <span class="math inline">\(b=1,2,... , B\)</span>, repeat:</p>
<ol style="list-style-type: lower-alpha">
<li>
<p>[ <span style="color:#00ea8d"><em>functional gradient descent</em></span> ] For each data point <span class="math inline">\(j=1,2,...,N\)</span>, compute the gradient descent <span class="math inline">\(r_b = (r_{b,1}, r_{b,2}, ...)^T\)</span> at stage <span class="math inline">\(b\)</span> based on the pre-specified loss function <span class="math inline">\(\mathcal{L}\)</span>:</p>
<p><span class="math display">\[
r_{b,j} =  - \left[ \frac{ \partial \mathcal{L}(y_j, f(x_j) }{\partial f(x_j)} \right]_{f= f^{GBT}_{(b-1)} }
\]</span></p>
</li>
<li><p>[ <span style="color:#00ea8d"><em>stochastic subsampling</em></span> ] sample a fraction <span class="math inline">\(\eta\)</span> of the training observations to growth the next trees using the subsample <span class="math inline">\(\tilde{X}, \tilde{r}_{b}\)</span></p></li>
<li><p>Fit a base regression tree <span class="math inline">\(f_{(b)}\)</span> with <span class="math inline">\(d\)</span> splits to the subsample training data (<span class="math inline">\(\tilde{X}\)</span>, <span class="math inline">\(\tilde{r}_{b}\)</span>)</p></li>
<li><p>[ <span style="color:#00ea8d"><em>shrunk weighting</em></span> ] Update <span class="math inline">\(f^{GBT}_{(b)}(x) = f^{GBT}_{(b-1)}(x) + \lambda f_{(b)}(x)\)</span></p></li>
</ol>
</li>
</ol>
<p>End the training</p>
<p><strong>Output</strong>: the boosted model <span class="math inline">\(\hat{f}^{GBT}(x) = f^{GBT}_{(B)}(x) = \sum_{b=0}^B \lambda^b f_{(b)}(x; d)\)</span></p>
<hr>
</div>
</div>
<div id="chapter-conclusion-1" class="section level2" number="4.4">
<h2>
<span class="header-section-number">4.4</span> Chapter Conclusion<a class="anchor" aria-label="anchor" href="#chapter-conclusion-1"><i class="fas fa-link"></i></a>
</h2>
<p>While the forecasting performance of individual trees is notoriously poor, ensembles of trees built by <span style="color:#00ea8d">boosting</span> are currently considered the most successful approach to classification and regression for tabular data.</p>
<blockquote>
<p>‚ÄúFinance is not a plug-and-play subject as it relates to machine learning.‚Äù - Marcos Lopez de Prado (2018), <a href="https://fintelligence-academy.github.io">Advances in Financial Machine Learning</a></p>
</blockquote>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="modern-factor-investing.html"><span class="header-section-number">3</span> Modern Factor Investing</a></div>
<div class="next"><a href="references.html">References</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#tree-based-models"><span class="header-section-number">4</span> Tree-based Models</a></li>
<li><a class="nav-link" href="#introduction-to-machine-learning-tree"><span class="header-section-number">4.1</span> Introduction to Machine Learning Tree üå≤</a></li>
<li><a class="nav-link" href="#boosting-and-additive-trees"><span class="header-section-number">4.2</span> Boosting and Additive Trees</a></li>
<li>
<a class="nav-link" href="#gradient-boosting-tree"><span class="header-section-number">4.3</span> Gradient Boosting Tree</a><ul class="nav navbar-nav"><li><a class="nav-link" href="#algorithm"><span class="header-section-number">4.3.1</span> Algorithm</a></li></ul>
</li>
<li><a class="nav-link" href="#chapter-conclusion-1"><span class="header-section-number">4.4</span> Chapter Conclusion</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
          
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Machine Learning for Quantitative Investment</strong>" was written by Fintelligence. It was last built on 2022-04-03.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
